{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fddba6d2-67f1-4f23-acf4-5be4f159cf64",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mticker\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmticker\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcartopy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m shapereader\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mean_squared_error\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstats\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pearsonr\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msignal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m correlate\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data49/yingying/miniconda/miniconda3/envs/data/lib/python3.13/site-packages/sklearn/__init__.py:73\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# `_distributor_init` allows distributors to run custom init code.\u001b[39;00m\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# For instance, for the Windows wheel, this is used to pre-load the\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# vcomp shared library runtime for OpenMP embedded in the sklearn/.libs\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401 E402\u001b[39;00m\n\u001b[32m     70\u001b[39m     __check_build,\n\u001b[32m     71\u001b[39m     _distributor_init,\n\u001b[32m     72\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clone  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_show_versions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m show_versions  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[32m     76\u001b[39m _submodules = [\n\u001b[32m     77\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcalibration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     78\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcluster\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    114\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcompose\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    115\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data49/yingying/miniconda/miniconda3/envs/data/lib/python3.13/site-packages/sklearn/base.py:19\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_estimator_html_repr\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _HTMLDocumentationLinkMixin, estimator_html_repr\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_metadata_requests\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _MetadataRequester, _routing_enabled\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_param_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data49/yingying/miniconda/miniconda3/envs/data/lib/python3.13/site-packages/sklearn/utils/__init__.py:15\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _joblib, metadata_routing\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_bunch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Bunch\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_chunking\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gen_batches, gen_even_slices\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_estimator_html_repr\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m estimator_html_repr\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Make _safe_indexing importable from here for backward compat as this particular\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# helper is considered semi-private and typically very useful for third-party\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# libraries that want to comply with scikit-learn's estimator API. In particular,\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# _safe_indexing was included in our public API documentation despite the leading\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# `_` in its name.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data49/yingying/miniconda/miniconda3/envs/data/lib/python3.13/site-packages/sklearn/utils/_chunking.py:11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_param_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Interval, validate_params\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mchunk_generator\u001b[39m(gen, chunksize):\n\u001b[32m     15\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Chunk generator, ``gen`` into lists of length ``chunksize``. The last\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[33;03m    chunk may have a length less than ``chunksize``.\"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data49/yingying/miniconda/miniconda3/envs/data/lib/python3.13/site-packages/sklearn/utils/_param_validation.py:14\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumbers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Integral, Real\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m csr_matrix, issparse\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvalidation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _is_arraylike_not_scalar\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data49/yingying/miniconda/miniconda3/envs/data/lib/python3.13/site-packages/scipy/sparse/__init__.py:300\u001b[39m\n\u001b[32m    294\u001b[39m \u001b[38;5;66;03m# Original code by Travis Oliphant.\u001b[39;00m\n\u001b[32m    295\u001b[39m \u001b[38;5;66;03m# Modified and extended by Ed Schofield, Robert Cimrman,\u001b[39;00m\n\u001b[32m    296\u001b[39m \u001b[38;5;66;03m# Nathan Bell, and Jake Vanderplas.\u001b[39;00m\n\u001b[32m    298\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m_warnings\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m300\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_base\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m    301\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_csr\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_csc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data49/yingying/miniconda/miniconda3/envs/data/lib/python3.13/site-packages/scipy/sparse/_base.py:5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"Base class for sparse matrices\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_sputils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (asmatrix, check_reshape_kwargs, check_shape,\n\u001b[32m      6\u001b[39m                        get_sum_dtype, isdense, isscalarlike,\n\u001b[32m      7\u001b[39m                        matrix, validateaxis, getdtype)\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_matrix\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m spmatrix\n\u001b[32m     11\u001b[39m __all__ = [\u001b[33m'\u001b[39m\u001b[33misspmatrix\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33missparse\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33msparray\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     12\u001b[39m            \u001b[33m'\u001b[39m\u001b[33mSparseWarning\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mSparseEfficiencyWarning\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data49/yingying/miniconda/miniconda3/envs/data/lib/python3.13/site-packages/scipy/sparse/_sputils.py:10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmath\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m prod\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msp\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_util\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m np_long, np_ulong\n\u001b[32m     13\u001b[39m __all__ = [\u001b[33m'\u001b[39m\u001b[33mupcast\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mgetdtype\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mgetdata\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33misscalarlike\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33misintlike\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     14\u001b[39m            \u001b[33m'\u001b[39m\u001b[33misshape\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33missequence\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33misdense\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mismatrix\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mget_sum_dtype\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     15\u001b[39m            \u001b[33m'\u001b[39m\u001b[33mbroadcast_shapes\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     17\u001b[39m supported_dtypes = [np.bool_, np.byte, np.ubyte, np.short, np.ushort, np.intc,\n\u001b[32m     18\u001b[39m                     np.uintc, np_long, np_ulong, np.longlong, np.ulonglong,\n\u001b[32m     19\u001b[39m                     np.float32, np.float64, np.longdouble,\n\u001b[32m     20\u001b[39m                     np.complex64, np.complex128, np.clongdouble]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data49/yingying/miniconda/miniconda3/envs/data/lib/python3.13/site-packages/scipy/_lib/_util.py:13\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TypeAlias, TypeVar\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_array_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m array_namespace, is_numpy, xp_size\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_docscrape\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FunctionDoc, Parameter\n\u001b[32m     17\u001b[39m AxisError: \u001b[38;5;28mtype\u001b[39m[\u001b[38;5;167;01mException\u001b[39;00m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data49/yingying/miniconda/miniconda3/envs/data/lib/python3.13/site-packages/scipy/_lib/_array_api.py:18\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnpt\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m array_api_compat\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marray_api_compat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     19\u001b[39m     is_array_api_obj,\n\u001b[32m     20\u001b[39m     size \u001b[38;5;28;01mas\u001b[39;00m xp_size,\n\u001b[32m     21\u001b[39m     numpy \u001b[38;5;28;01mas\u001b[39;00m np_compat,\n\u001b[32m     22\u001b[39m     device \u001b[38;5;28;01mas\u001b[39;00m xp_device,\n\u001b[32m     23\u001b[39m     is_numpy_namespace \u001b[38;5;28;01mas\u001b[39;00m is_numpy,\n\u001b[32m     24\u001b[39m     is_cupy_namespace \u001b[38;5;28;01mas\u001b[39;00m is_cupy,\n\u001b[32m     25\u001b[39m     is_torch_namespace \u001b[38;5;28;01mas\u001b[39;00m is_torch,\n\u001b[32m     26\u001b[39m     is_jax_namespace \u001b[38;5;28;01mas\u001b[39;00m is_jax,\n\u001b[32m     27\u001b[39m     is_array_api_strict_namespace \u001b[38;5;28;01mas\u001b[39;00m is_array_api_strict\n\u001b[32m     28\u001b[39m )\n\u001b[32m     30\u001b[39m __all__ = [\n\u001b[32m     31\u001b[39m     \u001b[33m'\u001b[39m\u001b[33m_asarray\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33marray_namespace\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33massert_almost_equal\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33massert_array_almost_equal\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     32\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mget_xp_devices\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     38\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mxp_take_along_axis\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mxp_unsupported_param_msg\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mxp_vector_norm\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     39\u001b[39m ]\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# To enable array API and strict array-like input validation\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data49/yingying/miniconda/miniconda3/envs/data/lib/python3.13/site-packages/scipy/_lib/array_api_compat/numpy/__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m * \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# from numpy import * doesn't overwrite these builtin names\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;28mabs\u001b[39m, \u001b[38;5;28mmax\u001b[39m, \u001b[38;5;28mmin\u001b[39m, \u001b[38;5;28mround\u001b[39m \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1410\u001b[39m, in \u001b[36m_handle_fromlist\u001b[39m\u001b[34m(module, fromlist, import_, recursive)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1412\u001b[39m, in \u001b[36m_handle_fromlist\u001b[39m\u001b[34m(module, fromlist, import_, recursive)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data49/yingying/miniconda/miniconda3/envs/data/lib/python3.13/site-packages/numpy/__init__.py:343\u001b[39m, in \u001b[36m__getattr__\u001b[39m\u001b[34m(attr)\u001b[39m\n\u001b[32m    341\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m random\n\u001b[32m    342\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m attr == \u001b[33m\"\u001b[39m\u001b[33mpolynomial\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpolynomial\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpolynomial\u001b[39;00m\n\u001b[32m    344\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m polynomial\n\u001b[32m    345\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m attr == \u001b[33m\"\u001b[39m\u001b[33mma\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1360\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1331\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:935\u001b[39m, in \u001b[36m_load_unlocked\u001b[39m\u001b[34m(spec)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1022\u001b[39m, in \u001b[36mexec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1118\u001b[39m, in \u001b[36mget_code\u001b[39m\u001b[34m(self, fullname)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1217\u001b[39m, in \u001b[36mget_data\u001b[39m\u001b[34m(self, path)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#%%\n",
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import glob\n",
    "import rasterio\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap,BoundaryNorm\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib import colors\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.collections import LineCollection\n",
    "# import imageio\n",
    "# import pygeodesy\n",
    "# from pygeodesy.ellipsoidalKarney import LatLon\n",
    "import folium\n",
    "# from shapely.geometry import Point,Polygon\n",
    "# from folium.plugins import FastMarkerCluster\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter\n",
    "import matplotlib.ticker as mticker\n",
    "from cartopy.io import shapereader\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.signal import correlate\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "import sys\n",
    "params_path = '/data42/yingying/HydroDA/src'\n",
    "#external python codes\n",
    "sys.path.append(params_path)\n",
    "import params_long01 as pm    # out_msm_round*\n",
    "from matplotlib.patches import Rectangle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a944eb7f-80ab-40eb-9394-41e95ce80b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "camadir  = pm.CaMa_dir()\n",
    "inputdir = pm.plot_dir()\n",
    "expname  = pm.runname(pm.mode())\n",
    "dahour   = pm.dahour()\n",
    "dahour_str = '{:02d}'.format(dahour)\n",
    "exp_plotdir = inputdir + '/exp_' + expname + dahour_str + '/'\n",
    "obsdir = pm.obs_dir(pm.real_mode())\n",
    "\n",
    "# simulation start from 20191007 but analysis start from 20191010\n",
    "# simulation start from 20240120 but analysis start from 20240201\n",
    "start_year,start_month,start_date,start_hour=pm.starttime() # Start year month date\n",
    "syyyy='%04d' % (start_year)\n",
    "if syyyy == '2019':\n",
    "    tstart = int(24*3)\n",
    "    tstart_da = int(24*3/dahour)\n",
    "elif syyyy == '2024':\n",
    "    tstart = int(24*12)\n",
    "    tstart_da = int(24*12/dahour)\n",
    "else:\n",
    "    tstart = 0\n",
    "    tstart_da = 0\n",
    "smm='%02d' % (start_month)\n",
    "sdd='%02d' % (start_date)\n",
    "shh='%02d' % (start_hour)\n",
    "stime = datetime(start_year,start_month,start_date,start_hour)\n",
    "end_year,end_month,end_date,end_hour=pm.endtime() # Start year month date\n",
    "eyyyy='%04d' % (end_year)\n",
    "emm='%02d' % (end_month)\n",
    "edd='%02d' % (end_date)\n",
    "ehh='%02d' % (end_hour)\n",
    "etime = datetime(end_year,end_month,end_date,end_hour)\n",
    "time_step = timedelta(hours=dahour)\n",
    "time_len = int((etime-stime).total_seconds() / 3600)\n",
    "time_len_da = int((etime-stime).total_seconds() / 3600 / dahour)\n",
    "\n",
    "path_alloc   = pm.alloc_dir()\n",
    "name_alloc   = '/wlv_2019h.xlsx'\n",
    "df     = pd.read_excel(path_alloc+name_alloc)\n",
    "df.rename(columns={df.columns[1]: 'latitude', df.columns[2]: 'longitude'}, inplace=True)\n",
    "id_info =  np.array(df.iloc[:,0])\n",
    "lat_obs =  np.array(df.iloc[:,1])\n",
    "lon_obs =  np.array(df.iloc[:,2])\n",
    "ix     = np.array(df.iloc[:,8])-1  # lon_ind\n",
    "iy     = np.array(df.iloc[:,9])-1  # lat_ind\n",
    "\n",
    "def read_bin2d(fname):\n",
    "    with open(fname, 'rb') as file:\n",
    "        data_bin = np.fromfile(file, dtype=np.float32)\n",
    "        data_re  = np.reshape(data_bin,(1320,1500))\n",
    "    return data_re\n",
    "def read_bin3d(fname,layer):\n",
    "    with open(fname, 'rb') as file:\n",
    "        data_bin = np.fromfile(file, dtype=np.float32)\n",
    "        data_re  = np.reshape(data_bin,(layer,1320,1500))\n",
    "    return data_re\n",
    "def read_bin_int(fname,layer):\n",
    "    with open(fname, 'rb') as file:\n",
    "        data_bin = np.fromfile(file, dtype=np.int32)\n",
    "        data_re  = np.reshape(data_bin,(layer,1320,1500))\n",
    "    return data_re\n",
    "def read_bin_station(fname):\n",
    "    with open(fname, 'rb') as file:\n",
    "        data_bin = np.fromfile(file, dtype=np.float32)\n",
    "    return data_bin\n",
    "def read_nc(fname,layer):\n",
    "    nf = nc.Dataset(fname,'r')\n",
    "    varname = nf.variables.keys()\n",
    "    varname = list(varname)\n",
    "    lat = np.array(nf.variables[varname[1]][:])\n",
    "    lon = np.array(nf.variables[varname[2]][:])\n",
    "    var = np.array(nf.variables[varname[layer]][:])\n",
    "    var = np.array(np.where(var>10**7,np.nan,var))\n",
    "    return var   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f932b468-559c-429f-968e-5da751c4e607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from 2019100100\n",
    "def read_prep(file):\n",
    "    nf = nc.Dataset(file,'r')\n",
    "    varname = nf.variables.keys()\n",
    "    varname = list(varname)\n",
    "    print(varname)\n",
    "    lon = np.array(nf.variables[varname[1]][:])\n",
    "    lat = np.array(nf.variables[varname[2]][:])\n",
    "    var = np.array(nf.variables[varname[3]][:])\n",
    "    var = var*3600 # unit: mm/h \n",
    "    var = np.array(np.where(var<-900,np.nan,var))\n",
    "    return lat,lon,var\n",
    "lat,lon,rainf = read_prep('/data42/yingying/data/matsiro_frc/tej_amedas2019.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d019011-8c1b-4dbf-b80b-fa87048a5f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_min,lat_max,lon_min,lon_max = 30,46,128,149\n",
    "def read_plot_tif(filename,lat_min,lat_max,lon_min,lon_max):\n",
    "    up_area  = np.zeros(((lat_max-lat_min)*3600,(lon_max-lon_min)*3600)) \n",
    "    lat_all  = np.zeros((lat_max-lat_min)*3600) \n",
    "    lon_all  = np.zeros((lon_max-lon_min)*3600)\n",
    "    filepath = pm.DA_dir()+'/src/plot/file/map/'+filename\n",
    "    tiff_files = glob.glob(os.path.join(filepath, '*.tif'))\n",
    "    for filenum in range(0,len(tiff_files)):\n",
    "        filen = tiff_files[filenum]\n",
    "        file_lon = int(filen[51:54])\n",
    "        file_lat = int(filen[48:50])\n",
    "        if (file_lat-lat_min>=0) &(file_lon-lon_min>=0) &(file_lat-lat_max<0) &(file_lon-lon_max<0):           \n",
    "            with rasterio.open(filen) as dataset:\n",
    "                tiff_data = dataset.read(1)\n",
    "                # be care !! when drawing the figure should convert latitude tiff[::-1,:]\n",
    "                up_area[(file_lat-lat_min)*3600:(file_lat-lat_min+1)*3600,(file_lon-lon_min)*3600:(file_lon-lon_min+1)*3600] = tiff_data[::-1,:]\n",
    "    return up_area\n",
    "up_plot = read_plot_tif('upa',30,46,128,149)\n",
    "#%% lower resolution\n",
    "size=120\n",
    "uparea_filter = np.where(up_plot<10**2,np.nan,up_plot)\n",
    "def low_resolution(var,size):\n",
    "    var_low = np.full((int((lat_max-lat_min)*3600/size),int((lon_max-lon_min)*3600/size)),np.nan)\n",
    "    for row in range(0,np.shape(var_low)[0]):\n",
    "        if (np.all(np.isnan(var[row*size:(row+1)*size,:]))==1):\n",
    "            continue\n",
    "        for col in range(0,np.shape(var_low)[1]):\n",
    "            var_low[row,col] = np.nanmean(var[row*size:(row+1)*size,col*size:(col+1)*size])\n",
    "    return var_low    \n",
    "up_low = low_resolution(uparea_filter,size) # for figure ploting\n",
    "lon_map_low = np.linspace(lon_min,lon_max,int((lon_max-lon_min)*3600/size))\n",
    "lat_map_low = np.linspace(lat_min,lat_max,int((lat_max-lat_min)*3600/size))\n",
    "lon2d_low_map, lat2d_low_map = np.meshgrid(lon_map_low, lat_map_low)\n",
    "\n",
    "# #%% read small tiff data\n",
    "lat_min,lat_max,lon_min,lon_max = 34,37,137,141\n",
    "up_area = read_plot_tif('upa',34,37,137,141)\n",
    "#%% lower resolution\n",
    "size=10\n",
    "def low_resolution(var,size):\n",
    "    var_low = np.full((int((lat_max-lat_min)*3600/size),int((lon_max-lon_min)*3600/size)),np.nan)\n",
    "    for row in range(0,np.shape(var_low)[0]):\n",
    "        if (np.all(np.isnan(var[row*size:(row+1)*size,:]))==1):\n",
    "            continue\n",
    "        for col in range(0,np.shape(var_low)[1]):\n",
    "            var_low[row,col] = np.nanmean(var[row*size:(row+1)*size,col*size:(col+1)*size])\n",
    "    return var_low    \n",
    "uparea_low = low_resolution(up_area,size)\n",
    "uparea_low = np.where(uparea_low<10,np.nan,uparea_low)\n",
    "lon_tiff = np.linspace(lon_min,lon_max,int((lon_max-lon_min)*3600/size))\n",
    "lat_tiff = np.linspace(lat_min,lat_max,int((lat_max-lat_min)*3600/size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4cffb0-2f17-4ab6-b2e4-147c15a5c74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% show the validation stations\n",
    "# obs_situ_dir = pm.alloc_dir() + '/'\n",
    "# 30 validation stations\n",
    "pm_path = pm.__file__\n",
    "obs_situ_dir = '/data42/yingying/HydroDA/Empirical_LocalPatch/MLIT/alloc/'\n",
    "if 'round' in pm_path:\n",
    "    file_select_lon = 'lon_select_index'+pm_path[-4]+'.txt'\n",
    "    file_select_lat = 'lat_select_index'+pm_path[-4]+'.txt'\n",
    "    file_valid_lon = 'lon_exclude_index'+pm_path[-4]+'.txt'\n",
    "    file_valid_lat = 'lat_exclude_index'+pm_path[-4]+'.txt'\n",
    "else:\n",
    "    file_select_lon = 'lon_select_index0.txt'\n",
    "    file_select_lat = 'lat_select_index0.txt'\n",
    "    file_valid_lon = 'lon_exclude_index0.txt'\n",
    "    file_valid_lat = 'lat_exclude_index0.txt'\n",
    "\n",
    "lony_select = np.array(np.loadtxt(obs_situ_dir+file_select_lon),dtype=int)\n",
    "latx_select = np.array(np.loadtxt(obs_situ_dir+file_select_lat),dtype=int)\n",
    "lony_valid = np.array(np.loadtxt(obs_situ_dir+file_valid_lon),dtype=int)\n",
    "latx_valid = np.array(np.loadtxt(obs_situ_dir+file_valid_lat),dtype=int)\n",
    "lat_select  = lat[latx_select-1]\n",
    "lon_select  = lon[lony_select-1]\n",
    "lat_valid  = lat[latx_valid-1]\n",
    "lon_valid  = lon[lony_valid-1]\n",
    "\n",
    "# numbers of assimilated pixels\n",
    "num_assim_path = pm.patch_filedir()+'/obs_patch/countnum.bin'\n",
    "assim_pixel   = read_bin_int(num_assim_path,2) \n",
    "assim_pixel_num = assim_pixel[0,:,:]\n",
    "\n",
    "assim_num = np.zeros(np.shape(lony_valid)[0])\n",
    "for i in range(0,np.shape(lony_valid)[0]):\n",
    "    assim_num[i] = assim_pixel_num[int(latx_valid[i]-1),int(lony_valid[i]-1)]\n",
    "\n",
    "lat_valid_plot = np.delete(latx_valid,assim_num==0)\n",
    "lon_valid_plot = np.delete(lony_valid,assim_num==0)\n",
    "assim_num_plot = np.delete(assim_num,assim_num==0)\n",
    "\n",
    "coords = np.stack((lat_valid_plot,lon_valid_plot),axis=1)\n",
    "_,unique_indices = np.unique(coords,axis=0,return_index = True)  # delete repeated elements\n",
    "unique_ind_sorted = np.sort(unique_indices)\n",
    "lat_unique = lat_valid_plot[unique_ind_sorted]\n",
    "lon_unique = lon_valid_plot[unique_ind_sorted]\n",
    "assim_unique = assim_num_plot[unique_ind_sorted]\n",
    "sorted_indices = np.lexsort((lon_unique, lat_unique))\n",
    "lat_valid_plot = lat_unique[sorted_indices]\n",
    "lon_valid_plot = lon_unique[sorted_indices]\n",
    "assim_num_plot = assim_unique[sorted_indices]\n",
    "\n",
    "#%% draw the select stations\n",
    "def draw_select_station(lon_valid,lat_valid,assim_num,title):\n",
    "    fig = plt.figure(dpi = 600,figsize=(7.5,4.5))\n",
    "    ax1 = fig.add_axes([0.1,0.1,0.8,0.8],projection=ccrs.PlateCarree())\n",
    "    ax1.set_axis_off()\n",
    "    # ax1.outline_patch.set_linewidth(0.35)\n",
    "    ax1.set_extent([128,149,30,46], ccrs.PlateCarree())\n",
    "    gl = ax1.gridlines(crs=ccrs.PlateCarree(), draw_labels=True,\n",
    "                    linewidth=1, color='gray', alpha=0, linestyle='-.')\n",
    "    ax1.coastlines(alpha=1.,linestyle='-',color = \"#3F3A3A\",lw=0.8,resolution='10m')\n",
    "    \n",
    "    # mask the other place\n",
    "    region_lon1 = [128.06, 128.06,138, 138 ]\n",
    "    region_lat1 = [41 , 45.95 , 45.95, 41  ]\n",
    "    region_lon2 = [128.06,128.06,131.4,131.4 ]\n",
    "    region_lat2 = [34.5,41,41,34.5]\n",
    "    ax1.fill(region_lon1, region_lat1, color='white', transform=ccrs.PlateCarree(), zorder=10)\n",
    "    ax1.fill(region_lon2, region_lat2, color='white', transform=ccrs.PlateCarree(), zorder=10)\n",
    "    \n",
    "    gl.top_labels   = False\n",
    "    gl.right_labels = False\n",
    "    gl.bottom_labels   = False\n",
    "    gl.left_labels = False\n",
    "    # con_plot1 = ax1.scatter(lon[ix],lat[iy],s=1,zorder=2,color='black',transform=ccrs.PlateCarree(),alpha=0.95)\n",
    "    con_plot1 = ax1.scatter(lon_select,lat_select,s=1,zorder=2,color='gray',transform=ccrs.PlateCarree(),alpha=0.95)\n",
    "    con_plot2 = ax1.scatter(lon_valid,lat_valid,s=assim_num*2,zorder=3,edgecolor='red',facecolors='none',lw=1.,transform=ccrs.PlateCarree())\n",
    "    con_plot2 = ax1.scatter(lon_valid,lat_valid,s=assim_num*2,zorder=3,edgecolor='red',facecolors='none',lw=1.,transform=ccrs.PlateCarree())\n",
    "    con_plot2 = ax1.scatter(137.10,34.89,s=20,zorder=3,edgecolor='red',facecolors='none',lw=1.,transform=ccrs.PlateCarree())\n",
    "    def color_bar(l,b,w,h):\n",
    "      rect = [l,b,w,h]\n",
    "      return cbar_ax\n",
    "    [l1,b1,w1,h1] = [0.63,0.12,0.01,0.25]\n",
    "    [l2,b2,w2,h2] = [0.63,0.32,0.01,0.25]\n",
    "    # cbar_ax2 = color_bar(l2,b2,w2,h2)\n",
    "    cmap_blue = ListedColormap(['skyblue']) \n",
    "    ax1.pcolormesh(lon2d_low_map,lat2d_low_map,up_low,cmap=cmap_blue,zorder=1, transform=ccrs.PlateCarree(),alpha=1.)  \n",
    "\n",
    "    # cb1.ax.set_title('NRMSE \\n $(m^{3}/s)$',fontdict=font_label)\n",
    "    # cb2 = plt.colorbar(con_plot2, cax=cbar_ax2,orientation=\"vertical\",shrink=0.5)\n",
    "    # ax1.set_title(title,loc='center')\n",
    "    if 'round' in pm_path:\n",
    "        plt.savefig(inputdir+title+pm_path[-4]+'.jpg', format='jpg',dpi=600)\n",
    "    else:\n",
    "        plt.savefig(inputdir+title+'.jpg', format='jpg',dpi=600)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "draw_select_station(lon_valid,lat_valid,assim_num,'/1.stations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9dfb48-0e8d-4c3f-b9c8-baeb407df70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% interactive map\n",
    "def plot_interactive():\n",
    "    situ_select_loc = np.zeros((np.shape(lat_select)[0],2))\n",
    "    situ_valid_loc  = np.zeros((np.shape(lat_valid)[0],2))\n",
    "    # lat,lon\n",
    "    situ_select_loc[:,0] = lat_select\n",
    "    situ_select_loc[:,1] = lon_select\n",
    "    situ_valid_loc[:,0] = lat_valid\n",
    "    situ_valid_loc[:,1] = lon_valid\n",
    "    lat_cen = 40\n",
    "    lon_cen = 141\n",
    "    my_map = folium.Map(location=(lat_cen,lon_cen),width=1000,height=600, zoom_start=10,attr=\"Map tiles by Stamen Design, under CC BY 3.0. Data by OpenStreetMap, under ODbL.\")\n",
    "    \n",
    "    # Save the map to an HTML file\n",
    "    ind = 0\n",
    "    for coord in situ_select_loc:\n",
    "        # read related data\n",
    "        folium.CircleMarker(\n",
    "            location=coord,\n",
    "            radius=1,\n",
    "            color='black',  # Assign color based on index\n",
    "            fill=True,\n",
    "            fill_color='black',\n",
    "            fill_opacity=0.6,\n",
    "        ).add_to(my_map)\n",
    "        ind = ind+1\n",
    "    for coord in situ_valid_loc:\n",
    "        # read related data\n",
    "        folium.CircleMarker(\n",
    "            location=coord,\n",
    "            radius=5,\n",
    "            color='red',  # Assign color based on index\n",
    "            fill=True,\n",
    "            fill_color='red',\n",
    "            fill_opacity=0.8,\n",
    "        ).add_to(my_map)\n",
    "        ind = ind+1\n",
    "    if 'round' in pm_path:\n",
    "        my_map.save('valid_map'+pm_path[-4]+'.html')\n",
    "    else:\n",
    "        my_map.save('valid_map.html')\n",
    "    return my_map\n",
    "\n",
    "my_map = plot_interactive()\n",
    "my_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b377cdb-c244-44ef-8e60-0c2c04406715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read DA output\n",
    "riv_outdir = inputdir + '/exp_' + expname + dahour_str + '/rivdph/'\n",
    "dis_outdir = inputdir + '/exp_' + expname + dahour_str + '/outflw/'\n",
    "new_row = ['da'+dahour_str]\n",
    "\n",
    "def read_obs(var):\n",
    "    current_time = stime\n",
    "    data_obs = np.full((time_len,len(lon_valid_plot)),np.nan)\n",
    "    ind = 0\n",
    "    while current_time<etime:\n",
    "        ctime = current_time.strftime('%Y%m%d%H')\n",
    "        if var=='wlv':\n",
    "            fname = obsdir+'/'+ctime+'.bin'\n",
    "        else:\n",
    "            fname = '/data42/yingying/obs/2019UST/dis/'+ctime+'.bin'\n",
    "        data_each = read_bin2d(fname)\n",
    "        data_each = np.where(data_each<-900,np.nan,data_each)\n",
    "        for sta_ind in range(0,len(lat_valid_plot)):\n",
    "            data_obs[ind,sta_ind] = data_each[int(lat_valid_plot[sta_ind])-1,int(lon_valid_plot[sta_ind])-1]\n",
    "        current_time += timedelta(hours=1)\n",
    "        ind = ind + 1\n",
    "    return data_obs\n",
    "elvmean  = read_bin2d(obsdir+'/elv_mean2019.bin')\n",
    "elvloc = np.full((len(lon_valid_plot)),np.nan)\n",
    "for sta_ind in range(len(lon_valid_plot)):\n",
    "    elvloc[sta_ind] = elvmean[int(lat_valid_plot[sta_ind])-1,int(lon_valid_plot[sta_ind])-1]\n",
    "    \n",
    "# move the files from pm.outdir() to ./yyyy/pm.outdir()\n",
    "if (pm.val_mode())==0:\n",
    "    outdir = pm.DA_dir()+'/out/'\n",
    "else:\n",
    "    outdir = pm.DA_dir()+'/'+str(syyyy)+'/out_valid_round'+str(pm.val_round(pm.val_mode()))\n",
    "    \n",
    "fmean = outdir + '/simmean/'+expname+'/C000.bin'\n",
    "simmean  = read_bin2d(fmean)\n",
    "\n",
    "# read rainfall\n",
    "if 'long' in pm.__file__:\n",
    "    prep_path = '/data42/yingying/data/matsiro_frc/'\n",
    "    # rainf = np.full((time_len,len(lon_valid_plot)),np.nan)\n",
    "    # ind = 0\n",
    "    # for mon in range(0,12):\n",
    "    #     mon_str  = '{:02d}'.format(mon+1)\n",
    "    #     lat,lon,rainf_mon = read_prep(prep_path+'tej_rain'+mon_str+'.nc')\n",
    "    #     if mon == 0:\n",
    "    #         # startdate start from 20240120\n",
    "    #         diff = stime-datetime(2024,1,1,0)\n",
    "    #         diff_hours = int(diff.total_seconds() / 3600)\n",
    "    #         rainf_mon  = rainf_mon[diff_hours:,:,:]\n",
    "    #     if mon == 11:\n",
    "    #         # enddate is 20241230\n",
    "    #         diff = datetime(2025,1,1,0)-etime\n",
    "    #         diff_hours = diff.total_seconds() / 3600\n",
    "    #         end_hour   = -int(np.shape(rainf_mon)[0]-diff_hours)-1\n",
    "    #         print('end_hour:',end_hour)\n",
    "    #         rainf_mon  = rainf_mon[:end_hour,:,:] \n",
    "    #     number = np.shape(rainf_mon)[0]\n",
    "    #     for sta_ind in range(len(lon_valid_plot)):\n",
    "    #         rainf[ind:int(ind+number),sta_ind] = rainf_mon[:,int(lat_valid_plot[sta_ind])-1,int(lon_valid_plot[sta_ind])-1]\n",
    "    #     ind = ind + number\n",
    "    rainf = np.load(prep_path+'rain2024.npy')\n",
    "else:\n",
    "    rainf = rainf[6*24:16*24,:,:]\n",
    "\n",
    "def read_stations(var):\n",
    "    if var == 'rivdph':\n",
    "        if 'long' in pm.__file__:\n",
    "            # load data to save time\n",
    "            data_obs_wlv = np.load('/data42/yingying/obs/2024UST/wlv/2024wlv.npy')\n",
    "        else:\n",
    "            data_obs_wlv = read_obs('wlv')\n",
    "        if 'round' in pm_path:\n",
    "            filepath = riv_outdir + 'valid/round'+pm_path[-4]+'/'\n",
    "        elif 'long' in pm_path:\n",
    "            filepath = riv_outdir + 'long/'\n",
    "        else:\n",
    "            filepath = riv_outdir + 'valid/'\n",
    "        data_obs = data_obs_wlv - elvloc\n",
    "    else:\n",
    "        data_obs = read_obs('dis')\n",
    "        # if 'long' in pm.__file__:\n",
    "        # np.save('/data42/yingying/obs/2024UST/dis/2024dis.npy',data_obs)\n",
    "        if 'round' in pm_path:\n",
    "            filepath = dis_outdir + 'valid/round'+pm_path[-4]+'/'\n",
    "        elif 'long' in pm_path:\n",
    "            filepath = dis_outdir + 'long/'\n",
    "        else:\n",
    "            filepath = dis_outdir + 'valid/'\n",
    "    da_station = np.full((time_len,pm.ens_mem(),len(lon_valid_plot)),np.nan)\n",
    "    sim_station = np.full((time_len,pm.ens_mem(),len(lon_valid_plot)),np.nan)\n",
    "    mean_station = np.full((len(lon_valid_plot)),np.nan)\n",
    "    obs_station  = data_obs\n",
    "    if 'long' in pm.__file__:\n",
    "        prep_station = rainf\n",
    "    else:\n",
    "        prep_station = np.full((time_len,len(lon_valid_plot)),np.nan)\n",
    "    for sta_ind in range(len(lon_valid_plot)):\n",
    "        if 'long' not in pm.__file__:\n",
    "            prep_station[:,sta_ind] = rainf[:,int(lat_valid_plot[sta_ind])-1,int(lon_valid_plot[sta_ind])-1]\n",
    "        mean_station[sta_ind] = simmean[int(lat_valid_plot[sta_ind])-1,int(lon_valid_plot[sta_ind])-1]\n",
    "        for ens in range(0,pm.ens_mem()):\n",
    "            fname = '{:04d}'.format(int(lat_valid_plot[sta_ind])) +  '{:04d}'.format(int(lon_valid_plot[sta_ind])) + '{:02d}'.format(ens+1)\n",
    "            da_station[:,ens,sta_ind] = read_bin_station(filepath+fname + 'A.bin')\n",
    "            sim_station[:,ens,sta_ind] = read_bin_station(filepath+fname + 'C.bin')\n",
    "    return data_obs,da_station,sim_station,obs_station,mean_station,prep_station\n",
    "data_obs_wlv,da_station,sim_station,obs_station,mean_station,prep_station = read_stations('rivdph')\n",
    "data_obs_dis,da_station_dis,sim_station_dis,obs_station_dis,mean_station_dis,prep_station_dis = read_stations('outflw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f2907b-a366-4a09-9401-df119d06ffb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# standard simulation is ens20 [-1]\n",
    "# plot station wlv validation\n",
    "def cal_rmse(y_true,y_pred,varname):\n",
    "    len_obs = np.shape(np.where(~np.isnan(y_true)))[1]\n",
    "    if \"Outflow\" in varname:\n",
    "        if np.shape(np.where(~np.isnan(y_true[24*2:24*4])))[1]<10:\n",
    "            rmse = np.nan\n",
    "        else:\n",
    "            rmse = np.sqrt(np.nanmean((y_pred-y_true)**2))\n",
    "    if \"WSE\" in varname:\n",
    "        if len_obs<int(time_len)/4:\n",
    "            rmse = np.nan\n",
    "        else:\n",
    "            rmse = np.sqrt(np.nanmean((y_pred-y_true)**2))\n",
    "    return rmse\n",
    "\n",
    "def draw_lines(varname,obs_station,sim_station,da_station,savename,new_row):\n",
    "    rows_old, cols = 7, 5\n",
    "    fig,axes = plt.subplots(rows_old, cols, dpi = 300,figsize=(25,15))\n",
    "    fig.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.25)\n",
    "    time    = np.arange(0,time_len,1)\n",
    "    valid_indices = [i for i in range(len(lon_valid_plot)) if not np.all(np.isnan(obs_station[tstart:, i]))]\n",
    "    if \"Outflow\" in varname:\n",
    "        # to remove the very small river stations\n",
    "        valid_indices = [i for i in range(len(lon_valid_plot)) if not (np.all(np.isnan(obs_station[tstart:, i])) or np.all(obs_station[tstart:, i] < 5))]\n",
    "    improve_rate = np.zeros(len(valid_indices))\n",
    "    num_valid = len(valid_indices)\n",
    "    rows = (num_valid + cols - 1) // cols\n",
    "    # plot stations\n",
    "    plot_idx = 0\n",
    "    lon_point = []\n",
    "    lat_point = []\n",
    "    lon_point_ind = []\n",
    "    lat_point_ind = []\n",
    "    obs_point_ind = []\n",
    "    sim_point_ind = []\n",
    "    da_point_ind = []\n",
    "    num_point = []\n",
    "    for ind, i in enumerate(valid_indices):\n",
    "        time_plot= time[tstart:]\n",
    "        obs_plot = obs_station[tstart:,i]\n",
    "        if \"WSE\" in varname:\n",
    "            sim_plot = sim_station[tstart:,:,i]-mean_station[i]\n",
    "            da_plot  = da_station[tstart:,:,i]-mean_station[i]\n",
    "        else:\n",
    "            sim_plot = sim_station[tstart:,:,i]\n",
    "            da_plot  = da_station[tstart:,:,i]\n",
    "        # RMSE\n",
    "        rmse_da = cal_rmse(obs_plot,np.nanmean(da_plot,axis=1),varname)\n",
    "        rmse_da_str = '{:.2f}'.format(rmse_da)\n",
    "        rmse_sim= cal_rmse(obs_plot,np.nanmean(sim_plot,axis=1),varname)  # compare with the average\n",
    "        # rmse_sim= cal_rmse(obs_plot,sim_plot[:,-1],varname)  # compare with open loop\n",
    "        rmse_sim_str= '{:.2f}'.format(rmse_sim)\n",
    "        if (np.isnan(rmse_da)) | (np.isnan(rmse_sim)):\n",
    "            improve_rate[ind] = np.nan\n",
    "            continue\n",
    "        if (np.shape(np.unique(da_plot))[0]<2):  # exclude the fail DA result\n",
    "            improve_rate[ind] = np.nan\n",
    "            continue\n",
    "        if (np.abs(np.nanmax(obs_plot))<0.1):  # exclude the gauges at small rivers\n",
    "            improve_rate[ind] = np.nan\n",
    "            continue            \n",
    "        # Rainfall\n",
    "        row, col = divmod(plot_idx,cols)\n",
    "        ax2 = axes[row,col].twinx()\n",
    "        ax2.invert_yaxis()\n",
    "        ax2.bar(time[tstart:],prep_station[tstart:,i],color='gray',alpha=0.8)\n",
    "        ax2.set_ylim(np.nanmax(prep_station[tstart:,i])+5,0)\n",
    "        ax2.spines['right'].set_color('gray')\n",
    "        ax2.yaxis.label.set_color('gray')\n",
    "        ax2.tick_params(axis='y',colors='gray') \n",
    "        # simulations\n",
    "        sim_point_ind = sim_point_ind + [sim_plot]\n",
    "        axes[row,col].plot(time_plot,sim_plot,'b-',linewidth=0.15,alpha=0.8)\n",
    "        # online DA\n",
    "        da_point_ind  = da_point_ind + [da_plot]\n",
    "        axes[row,col].plot(time_plot,da_plot,'r-',linewidth=1.25)\n",
    "        # print(np.unique(da_plot))\n",
    "        axes[row,col].set_xticks(np.arange(tstart+15,time_len,24))\n",
    "        axes[row,col].set_xlim(tstart,time_len)\n",
    "        # open loop\n",
    "        # axes[row,col].plot(time_plot,sim_plot[:,-1],'b-',linewidth=1.5)\n",
    "        axes[row,col].plot(time_plot,np.nanmean(sim_plot,axis=1),'b-',linewidth=1.5)\n",
    "        # obs\n",
    "        obs_point_ind = obs_point_ind + [obs_plot]\n",
    "        axes[row,col].plot(time_plot,obs_plot,color='green',linewidth=2.5)\n",
    "        lat_str = '{:.2f}'.format(lat[int(lat_valid_plot[i])-1])\n",
    "        # lat_str = '{:.2f}'.format(int(lat_valid_plot[i])-1)\n",
    "        lon_str = '{:.2f}'.format(lon[int(lon_valid_plot[i])-1])\n",
    "        # lon_str = '{:.2f}'.format(int(lon_valid_plot[i])-1)\n",
    "        if rmse_da < rmse_sim:\n",
    "            improve_rate[ind] = 1\n",
    "        axes[row,col].text(0.02,0.95,'('+lat_str+','+lon_str+')\\n' + 'obs number: '+str(int(assim_num_plot[i])),transform=axes[row,col].transAxes,fontsize=8, c='black',fontweight='bold', va='top', ha='left')\n",
    "        axes[row,col].text(0.9,0.95,'RMSE: '+rmse_da_str,transform=axes[row,col].transAxes,fontsize=8, c='red',fontweight='bold', va='top', ha='right')\n",
    "        axes[row,col].text(0.9,0.85,'RMSE: '+rmse_sim_str,transform=axes[row,col].transAxes,fontsize=8, c='blue',fontweight='bold', va='top', ha='right')\n",
    "        if col == 0:\n",
    "            axes[row,col].set_ylabel(varname)\n",
    "        if col == cols-1:\n",
    "            ax2.set_ylabel('Rain (mm/h)')\n",
    "        if 'ils01' in pm_path or 'round' or 'msm' in pm_path:\n",
    "            if 'WSE' in varname:\n",
    "                if row == 5: # parmas01\n",
    "                    dates = pd.date_range(start='2019-10-11 00:00',end='2019-10-17 08:00',freq='24h')\n",
    "                    axes[row,col].set_xticklabels(dates,rotation=25)\n",
    "                else:\n",
    "                    axes[row,col].set_xticklabels([],rotation=15)\n",
    "            else:\n",
    "                    if row == rows-1:\n",
    "                        dates = pd.date_range(start='2019-10-11 00:00',end='2019-10-17 08:00',freq='24h')\n",
    "                        axes[row,col].set_xticklabels(dates,rotation=25)\n",
    "                    else:\n",
    "                        axes[row,col].set_xticklabels([],rotation=15)     \n",
    "        if 'ils03' in pm_path:\n",
    "            if 'WSE' in varname:\n",
    "                if row == 5: # parmas03\n",
    "                    dates = pd.date_range(start='2019-10-11 00:00',end='2019-10-17 08:00',freq='24h')\n",
    "                    axes[row,col].set_xticklabels(dates,rotation=25)\n",
    "                else:\n",
    "                    axes[row,col].set_xticklabels([],rotation=15)\n",
    "            else:\n",
    "                    if row == 2:\n",
    "                        dates = pd.date_range(start='2019-10-11 00:00',end='2019-10-17 08:00',freq='24h')\n",
    "                        axes[row,col].set_xticklabels(dates,rotation=25)\n",
    "                    else:\n",
    "                        axes[row,col].set_xticklabels([],rotation=15) \n",
    "        plot_idx = plot_idx + 1\n",
    "        lon_point = lon_point + [lon[int(lon_valid_plot[i])-1]]\n",
    "        lat_point = lat_point + [lat[int(lat_valid_plot[i])-1]]\n",
    "        lon_point_ind  = lon_point_ind + [int(lon_valid_plot[i])-1]\n",
    "        lat_point_ind  = lat_point_ind + [int(lat_valid_plot[i])-1]\n",
    "        num_point = num_point + [assim_num_plot[i]]\n",
    "    for ind in range(plot_idx, rows_old * cols):\n",
    "        row, col = divmod(ind, cols)\n",
    "        fig.delaxes(axes[row, col])  # delete the empty \n",
    "    improve = np.nansum(improve_rate)/np.shape(np.where(~np.isnan(improve_rate)))[1]*100\n",
    "    print(np.nansum(improve_rate),np.shape(np.where(~np.isnan(improve_rate)))[1])\n",
    "    print('Overall number of stations:',np.shape(np.where(assim_num!=0))[1])\n",
    "    print('How many stations improve: ','{:.2f}'.format(improve),'%')\n",
    "    new_row = new_row + ['{:.2f}'.format(improve)]\n",
    "    if 'round' in pm_path:\n",
    "        plt.savefig(exp_plotdir+'2.cross_validation'+savename+pm_path[-4]+'.jpg', format='jpg',dpi=600)\n",
    "    elif 'long' in pm_path:\n",
    "        plt.savefig(exp_plotdir+'2.cross_validation'+savename+'999.jpg', format='jpg',dpi=600)\n",
    "    else:\n",
    "        plt.savefig(exp_plotdir+'2.cross_validation'+savename+'.jpg', format='jpg',dpi=600)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    return np.array(lon_point), np.array(lat_point), np.array(lon_point_ind), np.array(lat_point_ind), np.array(obs_point_ind), np.array(sim_point_ind), np.array(da_point_ind), np.array(num_point), new_row\n",
    "lon_point, lat_point, lon_point_ind, lat_point_ind, obs_point_ind, sim_point_ind, da_point_ind, num_point, new_row = draw_lines('Anomaly WSE (m)',obs_station,sim_station,da_station,'_wlv',new_row)\n",
    "\n",
    "# np.save(pm.plot_dir()+'/exp_'+expname+dahour_str+'/lon_point_ind.npy',lon_point_ind)\n",
    "# np.save(pm.plot_dir()+'/exp_'+expname+dahour_str+'/lat_point_ind.npy',lat_point_ind)\n",
    "# plot map\n",
    "draw_select_station(lon_point, lat_point, num_point,'/1.plot_stations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee8851c-9b67-4f15-a230-ac5210172e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print improvement of rolling selection experiment\n",
    "print('Average improvement of rolling result',np.nanmean([22/24,11/12,14/18,9/12,13/15]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8845c6-ca0e-4312-afda-f9c75757be7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# relationship between obs number and SVD, RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4716b2-22b0-4215-bf74-da6c409f63c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%% outflow validation\n",
    "lon_dis_point, lat_dis_point, lon_ind_test, lat_ind_test, obs_point_dis, sim_point_dis, da_point_dis, num_dis_point, new_row = draw_lines('Outflow (m${^3}$/s)',obs_station_dis,sim_station_dis,da_station_dis,'_HQ',new_row)\n",
    "draw_select_station(lon_dis_point, lat_dis_point, num_dis_point,'/1.plot_dis_stations')\n",
    "# draw_lines('Outflow (m${^3}$/s)',obs_station_dis,sim_station_dis,da_HQ,'_dis',new_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdd4b5e-68fb-4e49-a3cf-7c89798ff052",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%% NRMSE\n",
    "def draw_each_figure(obs_lat,obs_lon,varname):\n",
    "    situ_rmse  = np.full(np.shape(obs_lat)[0],np.nan)\n",
    "    situ_nrmse  = np.full(np.shape(obs_lat)[0],np.nan)\n",
    "    lat_plot   = np.full(np.shape(obs_lat)[0],1)\n",
    "    lon_plot   = np.full(np.shape(obs_lat)[0],1)\n",
    "    situ_imp   = np.full(np.shape(obs_lat)[0],np.nan)\n",
    "    peak_time_obs = np.full(np.shape(obs_lat)[0],np.nan)\n",
    "    loc_row = np.full(np.shape(obs_lat)[0],np.nan)\n",
    "    if 'round' in pm_path:\n",
    "        filepath = inputdir+'/exp_'+expname+dahour_str+'/round'+pm_path[-4]+'/'+varname+'/'\n",
    "    else:\n",
    "        filepath = inputdir+'/exp_'+expname+dahour_str+'/'+varname+'/'\n",
    "    # DA\n",
    "    filename = filepath+'dataA_mean01_20.nc'   # compare to the average of ensemble\n",
    "    da_mean = read_nc(filename,3)  # compare with the open loop result\n",
    "    # sim\n",
    "    filename = filepath+'dataC_mean01_20.nc'\n",
    "    # filename = filepath+'var00.nc'\n",
    "    if '00' in filename:\n",
    "        sim_mean = read_nc(filename,4)\n",
    "    else:\n",
    "        sim_mean = read_nc(filename,3)\n",
    "    for loc in range(0,np.shape(obs_lat)[0]):\n",
    "        loc_lat = obs_lat[loc]\n",
    "        loc_lon = obs_lon[loc]\n",
    "        loc_id  = id_info[loc]\n",
    "        #obs data_obs_wlv/data_obs_dis\n",
    "        if varname == 'rivdph':\n",
    "            obs_grid = data_obs_wlv[:,loc_lat,loc_lon]\n",
    "        else:\n",
    "            obs_grid = data_obs_dis[:,loc_lat,loc_lon]\n",
    "        if np.all(obs_grid)<-900:\n",
    "            continue\n",
    "        else:\n",
    "        # read data\n",
    "            if varname == 'rivdph':\n",
    "                loc_obs = obs_grid[tstart:]-elvmean[loc_lat,loc_lon]\n",
    "                loc_sim = sim_mean[tstart:,int(loc_lat),int(loc_lon)]-simmean[loc_lat,loc_lon]\n",
    "                loc_da  = da_mean[tstart:,loc_lat,loc_lon]-simmean[loc_lat,loc_lon]\n",
    "            else:\n",
    "                loc_obs = obs_grid[tstart:]\n",
    "                loc_sim = sim_mean[tstart:,loc_lat,loc_lon]\n",
    "                loc_da  = da_mean[tstart:,loc_lat,loc_lon]\n",
    "            if np.all(loc_da[10:30]<10**(-8))==True:\n",
    "                loc_da = np.full(time_len-tstart,np.nan)\n",
    "            # remove no obs stations   \n",
    "            mse_sim = np.nanmean((loc_obs-loc_sim)**2)\n",
    "            mse_da  = np.nanmean((loc_obs-loc_da)**2)\n",
    "            if np.isnan(mse_sim) | np.isnan(mse_da) | np.all(np.isnan(loc_da)):\n",
    "                continue\n",
    "            rmse = np.sqrt(mse_sim) - np.sqrt(mse_da)\n",
    "            if (np.shape(loc_obs)[0]<5) | (np.shape(loc_sim)[0]<5) | (np.shape(loc_da)[0]<5):\n",
    "                rmse = np.nan\n",
    "                nrmse = np.nan\n",
    "                imp = np.nan\n",
    "            else:\n",
    "                var_max  = np.nanmax(loc_obs)\n",
    "                var_min  = np.nanmin(loc_obs)\n",
    "                var_range = var_max-var_min\n",
    "                if var_range ==0:\n",
    "                    nrmse = np.nan\n",
    "                else:\n",
    "                    nrmse = rmse/var_range\n",
    "                peak_time_obs_each = np.nanargmax(loc_obs)\n",
    "                # if nrmse<-0.2:\n",
    "                #     print(nrmse,rmse,loc_lat,loc_lon)\n",
    "                imp = rmse/np.sqrt(mse_sim)            \n",
    "            # store nrmse, lat, loc\n",
    "            situ_rmse[loc]=rmse\n",
    "            situ_nrmse[loc]=nrmse\n",
    "            situ_imp[loc]=imp\n",
    "            lat_plot[loc]=int(loc_lat)\n",
    "            lon_plot[loc]=int(loc_lon)\n",
    "            loc_row[loc]=loc\n",
    "            peak_time_obs[loc]=peak_time_obs_each\n",
    "    return situ_rmse,situ_nrmse,situ_imp,peak_time_obs,loc_row,lat_plot,lon_plot\n",
    "# compare with rivdph\n",
    "situ_rmse_wlv,situ_nrmse_wlv,situ_imp_wlv,peak_time_obs_wlv,loc_row_wlv,lat_plot_wlv,lon_plot_wlv = draw_each_figure(iy,ix,'rivdph')\n",
    "# compare with outflow\n",
    "situ_rmse_dis,situ_nrmse_dis,situ_imp_dis,peak_time_obs_dis,loc_row_dis,lat_plot_dis,lon_plot_dis = draw_each_figure(iy,ix,'outflw')\n",
    "\n",
    "def performance(situ_rmse,situ_nrmse,situ_imp,new_row):\n",
    "    if (np.shape(np.where(situ_nrmse>0))[1]+np.shape(np.where(situ_nrmse<0))[1]) == 0:\n",
    "        nrmse_pos = np.nan\n",
    "    else:\n",
    "        nrmse_pos = np.shape(np.where(situ_nrmse>0))[1]/(np.shape(np.where(situ_nrmse>0))[1]+np.shape(np.where(situ_nrmse<0))[1])\n",
    "    nrmse_neg = 1-nrmse_pos\n",
    "    print('better',nrmse_pos*100,'worse',nrmse_neg*100)\n",
    "    print('average performance',np.nanmean(situ_rmse),np.nanmean(situ_nrmse),np.nanmean(situ_imp)*100)\n",
    "    values = [nrmse_pos*100, nrmse_neg*100, np.nanmean(situ_rmse), np.nanmean(situ_nrmse), np.nanmean(situ_imp)*100]\n",
    "    new_row += ['{:.2f}'.format(v) for v in values]\n",
    "    return new_row\n",
    "# rivdph\n",
    "new_row = performance(situ_rmse_wlv,situ_nrmse_wlv,situ_imp_wlv,new_row)\n",
    "# outflow\n",
    "new_row = performance(situ_rmse_dis,situ_nrmse_dis,situ_imp_dis,new_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798f4457-5954-49d6-b8c4-9a5f3514232c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the values\n",
    "column = [\"da_hours\", \"wlv (%)\", \"dis (%)\", \n",
    "    \"wlv_better (%)\", \"wlv_worse (%)\", \"wlv_rmse\", \n",
    "    \"wlv_nrmse\", \"wlv_imp (%)\", \n",
    "    \"dis_better (%)\", \"dis_worse (%)\", \"dis_rmse\", \n",
    "    \"dis_nrmse\", \"dis_imp (%)\"]\n",
    "# df = pd.DataFrame(columns=column)\n",
    "# df.to_csv(pm.plot_dir()+'/result.csv',index=False)\n",
    "# new_row_pd = pd.DataFrame([new_row],columns=df.columns)\n",
    "# new_row_pd.to_csv(pm.plot_dir()+'/result.csv', mode=\"a\", index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7e7fbb-c6d2-4552-a647-eb36c4f2b6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% draw the map\n",
    "def draw_rmse(title,var_name,lon_plot,lat_plot,situ_rmse,vmin,vmax):\n",
    "    fig = plt.figure(dpi = 600,figsize=(7.5,4.5))\n",
    "    ax1 = fig.add_axes([0.1,0.1,0.8,0.8],projection=ccrs.PlateCarree())\n",
    "    ax1.set_axis_off()\n",
    "    # ax1.outline_patch.set_linewidth(0.35)\n",
    "    ax1.set_extent([128,149,30,46], ccrs.PlateCarree())\n",
    "    gl = ax1.gridlines(crs=ccrs.PlateCarree(), draw_labels=True,\n",
    "                    linewidth=1, color='gray', alpha=0, linestyle='-.')\n",
    "    ax1.coastlines(alpha=1.,linestyle='-',color = \"#3F3A3A\",lw=0.8,resolution='10m')\n",
    "    \n",
    "    # mask the other place\n",
    "    region_lon1 = [128.06, 128.06,138, 138 ]\n",
    "    region_lat1 = [41 , 45.95 , 45.95, 41  ]\n",
    "    region_lon2 = [128.06,128.06,131.4,131.4 ]\n",
    "    region_lat2 = [34.5,41,41,34.5]\n",
    "    ax1.fill(region_lon1, region_lat1, color='white', transform=ccrs.PlateCarree(), zorder=10)\n",
    "    ax1.fill(region_lon2, region_lat2, color='white', transform=ccrs.PlateCarree(), zorder=10)\n",
    "    \n",
    "    gl.top_labels   = False\n",
    "    gl.right_labels = False\n",
    "    gl.bottom_labels   = False\n",
    "    gl.left_labels = False\n",
    "    # gl.xlocator = mticker.FixedLocator([130,135,140,145])\n",
    "    # gl.ylocator = mticker.FixedLocator([30,35,40,45])\n",
    "    # gl.xlabel_style = {'size': 8,'color':\"#3F3A3A\",'weight':'normal'}\n",
    "    # gl.ylabel_style = {'size': 8,'color':\"#3F3A3A\",'weight':'normal'}\n",
    "    # lon_formatter = LongitudeFormatter(zero_direction_label=True)\n",
    "    # lat_formatter = LatitudeFormatter()\n",
    "    # ax1.xaxis.set_major_formatter(lon_formatter)\n",
    "    # ax1.yaxis.set_major_formatter(lat_formatter)\n",
    "    norm = mcolors.TwoSlopeNorm(vmin=min(situ_rmse), vcenter=0, vmax=-min(situ_rmse))\n",
    "    con_plot2 = ax1.scatter(lon[lon_plot],lat[lat_plot],c=situ_rmse,s=1.5,zorder=2,cmap='coolwarm',transform=ccrs.PlateCarree(),vmin=vmin,vmax=vmax,alpha=0.8)\n",
    "    # station highlight  \n",
    "    # ax1.scatter(lon[918],lat[556],c='black',alpha=0.5,zorder=3,transform=ccrs.PlateCarree(),marker='*',s=200)\n",
    "    def color_bar(l,b,w,h):\n",
    "      rect = [l,b,w,h]\n",
    "      cbar_ax = fig.add_axes(rect)\n",
    "      return cbar_ax\n",
    "    [l1,b1,w1,h1] = [0.63,0.12,0.01,0.25]\n",
    "    [l2,b2,w2,h2] = [0.63,0.32,0.01,0.25]\n",
    "    cbar_ax2 = color_bar(l2,b2,w2,h2)\n",
    "    if var_name == 'rivdph':\n",
    "        cb_title = '$(m)$'\n",
    "    else:\n",
    "        cb_title = '$(m^{3}/s)$'\n",
    "    cb2 = plt.colorbar(con_plot2, cax=cbar_ax2,orientation=\"vertical\",shrink=0.5)\n",
    "    cb2.ax.set_title(cb_title,fontsize=8)\n",
    "    ax1.set_title(title,loc='center')\n",
    "    if 'round' in pm_path:\n",
    "        plt.savefig(exp_plotdir+'3.map_'+title+pm_path[-4]+'.jpg', format='jpg',dpi=600)\n",
    "    elif 'long' in pm_path:\n",
    "        plt.savefig(exp_plotdir+'3.map_'+title+pm_path[-4]+'.jpg', format='jpg',dpi=600)\n",
    "        \n",
    "    else:\n",
    "        plt.savefig(exp_plotdir+'3.map_'+title+'.jpg', format='jpg',dpi=600)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "# wlv\n",
    "draw_rmse('Rivdph_RMSE','rivdph',lon_plot_wlv,lat_plot_wlv,situ_rmse_wlv,-1,1)   \n",
    "draw_rmse('Rivdph_nRMSE','rivdph',lon_plot_wlv,lat_plot_wlv,situ_nrmse_wlv,-0.8,0.8)   \n",
    "draw_rmse('Rivdph_Improvement','rivdph',lon_plot_wlv,lat_plot_wlv,situ_imp_wlv,0,1)  \n",
    "print('-------------------')\n",
    "# dis\n",
    "draw_rmse('Outflow_RMSE','outflow',lon_plot_dis,lat_plot_dis,situ_rmse_dis,-np.nanmax(situ_rmse_dis)*0.6,np.nanmax(situ_rmse_dis)*0.6) \n",
    "draw_rmse('Outflow_nRMSE','outflow',lon_plot_dis,lat_plot_dis,situ_nrmse_dis,-0.4,0.4)   \n",
    "draw_rmse('Outflow_Improvement','outflow',lon_plot_dis,lat_plot_dis,situ_imp_dis,0,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafabeed-f0a6-47c3-9d9e-ad1364f74ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% draw observed-outflow\n",
    "def cal_HQ():\n",
    "    cal = 0\n",
    "    Q_obs = np.full((np.shape(da_station_dis)[0],np.shape(da_station_dis)[1],np.shape(da_station_dis)[2]),np.nan)\n",
    "    for station_ind in range(0,np.shape(obs_station)[1]):\n",
    "        if np.shape(np.where(~np.isnan(obs_station_dis[tstart:,station_ind])))[1]<time_len/4:\n",
    "            continue\n",
    "        obs_wlv = obs_station[tstart:,station_ind]\n",
    "        obs_dis = obs_station_dis[tstart:,station_ind]\n",
    "        mask = ~np.isnan(obs_wlv) & ~np.isnan(obs_dis)\n",
    "        obs_wlv_new = obs_wlv[mask]\n",
    "        obs_dis_new = obs_dis[mask]\n",
    "        coeffi = np.polyfit(obs_wlv_new,obs_dis_new,2) # second-order polynomic\n",
    "        a, b, c = coeffi\n",
    "        Q_fit  = a*obs_wlv_new**2 + b*obs_wlv_new + c\n",
    "        R,_ = pearsonr(obs_dis_new,Q_fit)\n",
    "        for ens_ind in range(0,np.shape(da_station)[1]):\n",
    "            da_wlv  = da_station[tstart:,ens_ind,station_ind]\n",
    "            Q_obs[tstart:,ens_ind,station_ind]  = a*da_wlv**2 + b*da_wlv + c\n",
    "        if R<0.85:\n",
    "            continue\n",
    "        cal = cal + 1\n",
    "    return Q_obs\n",
    "\n",
    "da_station_disnew = cal_HQ()\n",
    "# plot the fitting H-Q figure\n",
    "lon_test, lat_test, lon_ind_test, lat_ind_test, obs_point_test, sim_point_test, da_point_test, num_test, new_test = draw_lines('Outflow (m${^3}$/s)',obs_station_dis,sim_station_dis,da_station_disnew,'_dis',new_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1069893-fdd4-46b1-bbce-86752e9c88ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read 24h data\n",
    "# make by code /data42/yingying/HydroDA/src/plot/7.make_24forecast.py\n",
    "# [ens,time_len-24,stations,24hours]\n",
    "wlv_da  = np.load(pm.plot_dir()+'/exp_'+expname+dahour_str+'/rivdph/da_24fore.npy')\n",
    "wlv_sim = np.load(pm.plot_dir()+'/exp_'+expname+dahour_str+'/rivdph/sim_24fore.npy')\n",
    "dis_da  = np.load(pm.plot_dir()+'/exp_'+expname+dahour_str+'/outflw/da_24fore.npy')\n",
    "dis_sim = np.load(pm.plot_dir()+'/exp_'+expname+dahour_str+'/outflw/sim_24fore.npy')\n",
    "lon_point_ind = np.load(pm.plot_dir()+'/exp_'+expname+dahour_str+'/lon_point_ind.npy')\n",
    "lat_point_ind = np.load(pm.plot_dir()+'/exp_'+expname+dahour_str+'/lat_point_ind.npy')\n",
    "# original runoff forcing: dis_da_all[-1,:,:,:]\n",
    "dis_da_all = np.full((20, int(216/dahour), np.shape(lon_point_ind)[0], 24),np.nan)\n",
    "dis_da_all[:,int(24*2/dahour):,:,:] = dis_da\n",
    "wlv_da_all = np.full((20, int(216/dahour), np.shape(lon_point_ind)[0], 24),np.nan)\n",
    "wlv_da_all[:,int(24*2/dahour):,:,:] = wlv_da\n",
    "wlv_da = wlv_da_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c325df-0dea-4d5b-b4b3-d8f643ac9633",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot station 24 hour forecast wlv validation\n",
    "def plot_gradient_line(ax, x, y, cmap, linewidth=1.0, alpha=0.8, vmin=None, vmax=None,add_colorbar=False,cbar_label='leading time',fig=None):\n",
    "    x = np.asarray(x)\n",
    "    y = np.asarray(y)\n",
    "    if len(x) != len(y):\n",
    "        raise ValueError(\"x and y must be the same length\")\n",
    "    if len(x) < 2:\n",
    "        return  # Don't do anything\n",
    "    if isinstance(cmap, str):\n",
    "        cmap = plt.colormaps.get_cmap(cmap)\n",
    "        colors = cmap(np.linspace(0.15, 0.85, len(x)-1))\n",
    "    if vmin is None:\n",
    "        vmin = 0\n",
    "    if vmax is None:\n",
    "        vmax = len(x) - 2\n",
    "    norm = mcolors.Normalize(vmin=vmin, vmax=vmax)\n",
    "    points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "    segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "    lc = LineCollection(segments, cmap=cmap, norm=norm,\n",
    "                        linewidth=linewidth, alpha=alpha)\n",
    "    lc.set_array(np.arange(len(x)-1)) \n",
    "    ax.add_collection(lc)\n",
    "    if add_colorbar and fig is not None:\n",
    "        sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "        sm.set_array([])  # no need for the real value\n",
    "        ticks = np.linspace(0,len(x)-1,len(x)).astype(int)+1\n",
    "        cbar = fig.colorbar(sm, ax=ax, pad=0.01)\n",
    "        cbar.set_label(cbar_label) \n",
    "        cbar.ax.tick_params(labelsize=15)  \n",
    "        cbar.ax.yaxis.set_ticks_position('right') \n",
    "        cbar.set_ticks(ticks-1)\n",
    "        cbar.set_ticklabels(ticks)\n",
    "        cbar.ax.set_position([0.92, 0.5, 0.02, 0.4]) \n",
    "    return lc \n",
    "\n",
    "def draw_fore_lines(varname,obs_station,sim_station,da_station,wlv_da,savename,station_lat,station_lon,station_idx):\n",
    "    rows_old, cols = 7, 5\n",
    "    fig,axes = plt.subplots(rows_old, cols, dpi = 300,figsize=(25,15))\n",
    "    fig.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.25)\n",
    "    time    = np.arange(0,time_len,1)\n",
    "    valid_indices = [i for i in range(len(lon_valid_plot)) if not np.all(np.isnan(obs_station[tstart:, i]))]\n",
    "    if \"Outflow\" in varname:\n",
    "        # to remove the very small river stations\n",
    "        valid_indices = [i for i in range(len(lon_valid_plot)) if not (np.all(np.isnan(obs_station[tstart:, i])) or np.all(obs_station[tstart:, i] < 5))]\n",
    "    improve_rate = np.zeros(len(valid_indices))\n",
    "    num_valid = len(valid_indices)\n",
    "    rows = (num_valid + cols - 1) // cols\n",
    "    # plot stations\n",
    "    plot_idx = 0\n",
    "    dis_use_ind = 0\n",
    "    lon_point = []\n",
    "    lat_point = []\n",
    "    lon_point_ind = []\n",
    "    lat_point_ind = []\n",
    "    num_point = []\n",
    "    dis_da_use = np.full((np.shape(wlv_da)[0],np.shape(wlv_da)[1],np.shape(obs_point_dis)[0],np.shape(wlv_da)[3]),np.nan)\n",
    "    for ind, i in enumerate(valid_indices):\n",
    "        time_plot= time[tstart:]\n",
    "        obs_plot = obs_station[tstart:,i]\n",
    "        if \"WSE\" in varname:\n",
    "            sim_plot = sim_station[tstart:,:,i]-mean_station[i]\n",
    "            da_plot  = da_station[tstart:,:,i]-mean_station[i]\n",
    "        else:\n",
    "            sim_plot = sim_station[tstart:,:,i]\n",
    "            da_plot  = da_station[tstart:,:,i]\n",
    "        # RMSE (compare the original data)\n",
    "        # da_plot_singleline  = da_plot[:,-1]\n",
    "        # sim_plot_singleline = sim_plot[:,-1]\n",
    "        da_plot_singleline  = np.nanmean(da_plot,axis=1)\n",
    "        sim_plot_singleline = np.nanmean(sim_plot,axis=1)\n",
    "        rmse_da = cal_rmse(obs_plot,da_plot_singleline,varname)\n",
    "        rmse_da_str = '{:.2f}'.format(rmse_da)\n",
    "        rmse_sim= cal_rmse(obs_plot,sim_plot_singleline,varname)\n",
    "        rmse_sim_str= '{:.2f}'.format(rmse_sim)\n",
    "        if (np.isnan(rmse_da)) | (np.isnan(rmse_sim)):\n",
    "            improve_rate[ind] = np.nan\n",
    "            continue\n",
    "        if (np.shape(np.unique(da_plot))[0]<2):  # exclude the fail DA result\n",
    "            improve_rate[ind] = np.nan\n",
    "            continue\n",
    "        if (np.abs(np.nanmax(obs_plot))<0.1):  # exclude the gauges at small rivers\n",
    "            improve_rate[ind] = np.nan\n",
    "            continue            \n",
    "        # Rainfall\n",
    "        row, col = divmod(plot_idx,cols)\n",
    "        ax2 = axes[row,col].twinx()\n",
    "        ax2.invert_yaxis()\n",
    "        ax2.bar(time[tstart:],prep_station[tstart:,i],color='gray',alpha=0.8)\n",
    "        ax2.set_ylim(np.nanmax(prep_station[tstart:,i])+5,0)\n",
    "        ax2.spines['right'].set_color('gray')\n",
    "        ax2.yaxis.label.set_color('gray')\n",
    "        ax2.tick_params(axis='y',colors='gray') \n",
    "        # obs\n",
    "        axes[row,col].plot(time_plot,obs_plot,color='green',linewidth=2.)\n",
    "        axes[row,col].set_xticks(np.arange(tstart+15,time_len,24))\n",
    "        axes[row,col].set_xlim(tstart,time_len)\n",
    "        lat_str = '{:.2f}'.format(lat[int(lat_valid_plot[i])-1])\n",
    "        # lat_str = '{:.2f}'.format(int(lat_valid_plot[i])-1)\n",
    "        lon_str = '{:.2f}'.format(lon[int(lon_valid_plot[i])-1])\n",
    "        # lon_str = '{:.2f}'.format(int(lon_valid_plot[i])-1)\n",
    "        # use redish color to plot lines\n",
    "        # simulations\n",
    "        axes[row,col].plot(time_plot,sim_plot_singleline,'b-',linewidth=1.5)\n",
    "        if \"WSE\" in varname:\n",
    "            station_lat = station_lat + [int(lat_valid_plot[i])-1]\n",
    "            station_lon = station_lon + [int(lon_valid_plot[i])-1]\n",
    "            station_idx  = station_idx  + [plot_idx]\n",
    "        for time_ind in range(tstart_da,np.shape(wlv_da)[1],1):\n",
    "            # for ens_ind in range(0,pm.ens_mem()):\n",
    "            #     # online DA\n",
    "            #     axes[row,col].plot(np.arange(time_ind,time_ind+24,1),wlv_sim[ens_ind,time_ind,plot_idx,:],'b-',linewidth=0.25)\n",
    "            x_time = np.arange(time_ind*dahour,time_ind*dahour+24,1)\n",
    "            if \"Outflow\" in varname:\n",
    "                station_ind = np.where((station_lat==int(lat_valid_plot[i]-1)) & (station_lon==int(lon_valid_plot[i]-1)))[0][0]\n",
    "                y_value = np.nanmean(wlv_da[:,time_ind,int(station_idx[int(station_ind)]),:],axis=0)\n",
    "                # only store the data for stations have water discharge observations\n",
    "                dis_da_use[:,time_ind,dis_use_ind,:] =  wlv_da[:,time_ind,int(station_idx[int(station_ind)]),:]\n",
    "            else:\n",
    "                y_value = np.nanmean(wlv_da[:,time_ind,plot_idx,:],axis=0)\n",
    "            plot_gradient_line(axes[row, col], x_time, y_value, 'Reds_r', linewidth=0.5,add_colorbar=(time_ind == tstart),fig=fig)        \n",
    "        if rmse_da < rmse_sim:\n",
    "            improve_rate[ind] = 1\n",
    "        axes[row,col].text(0.02,0.95,'('+lat_str+','+lon_str+')\\n' + 'obs number: '+str(int(assim_num_plot[i])),transform=axes[row,col].transAxes,fontsize=8, c='black',fontweight='bold', va='top', ha='left')\n",
    "        if col == 0:\n",
    "            axes[row,col].set_ylabel(varname)\n",
    "        if col == cols-1:\n",
    "            ax2.set_ylabel('Rain (mm/h)')\n",
    "        if row != rows-1:\n",
    "            axes[row,col].set_xticklabels([],rotation=15)\n",
    "        if (\"WSE\" in varname) & (row == 5):\n",
    "            dates = pd.date_range(start='2019-10-11 00:00',end='2019-10-17 08:00',freq='24h')\n",
    "            axes[row,col].set_xticklabels(dates,rotation=25)\n",
    "        if (\"Outflow\" in varname) & (row == 3):\n",
    "            dates = pd.date_range(start='2019-10-11 00:00',end='2019-10-17 08:00',freq='24h')\n",
    "            axes[row,col].set_xticklabels(dates,rotation=25)            \n",
    "        plot_idx = plot_idx + 1\n",
    "        lon_point = lon_point + [lon[int(lon_valid_plot[i])-1]]\n",
    "        lat_point = lat_point + [lat[int(lat_valid_plot[i])-1]]\n",
    "        lon_point_ind  = lon_point_ind + [int(lon_valid_plot[i])-1]\n",
    "        lat_point_ind  = lat_point_ind + [int(lat_valid_plot[i])-1]\n",
    "        num_point = num_point + [assim_num_plot[i]]\n",
    "        dis_use_ind = dis_use_ind + 1\n",
    "    for ind in range(plot_idx, rows_old * cols):\n",
    "        row, col = divmod(ind, cols)\n",
    "        fig.delaxes(axes[row, col])  # delete the empty \n",
    "    improve = np.nansum(improve_rate)/np.shape(np.where(~np.isnan(improve_rate)))[1]*100\n",
    "    print('Overall number of stations:',np.shape(np.where(assim_num!=0))[1])\n",
    "    print('How many stations improve: ','{:.2f}'.format(improve),'%')\n",
    "    plt.savefig(exp_plotdir+'2.fore_validation'+savename+'.jpg', format='jpg',dpi=600)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    if \"WSE\" in varname:\n",
    "        return np.array(station_lat),np.array(station_lon),np.array(station_idx),None\n",
    "    else:\n",
    "        return None,None,None,dis_da_use\n",
    "station_lat,station_lon,station_idx,wlv_da_test = draw_fore_lines('Anomaly WSE (m)',obs_station,sim_station,da_station,wlv_da,'_wlv',[],[],[])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91e7069-3139-4b5c-9727-24d6a8ee771d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dis_da_use: only store the water discharge at stations with observation (12 stations)\n",
    "# wlv_da: WSE at stations with observations (24 stations)\n",
    "lat_test,lon_test,idx_test,dis_da_use = draw_fore_lines('Outflow (m${^3}$/s)',obs_station_dis,sim_station_dis,da_station_dis,dis_da_all,'_dis',station_lat,station_lon,station_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc86eace-e768-4531-a4f6-d393228da9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(wlv_sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78466533-b174-4f40-94de-ee4dfec0390b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calculate KGE, Timing error, Person Correlation for various leading time\n",
    "def cal_single_metric(obs_data,sim_data):\n",
    "    if np.shape(obs_data)[0] != np.shape(sim_data)[0]:\n",
    "        print('shape:',np.shape(obs_data)[0],np.shape(sim_data)[0])\n",
    "    mask = ~np.isnan(obs_data) & ~np.isnan(sim_data)\n",
    "    if np.shape(sim_data)[0]>20:\n",
    "        obs = obs_data[mask]\n",
    "        sim = sim_data[mask]\n",
    "    else:\n",
    "        obs = obs_data\n",
    "        sim = sim_data\n",
    "    if ((np.all(mask) == True) & (np.shape(sim)[0]>1)) | (np.shape(sim)[0]>20):\n",
    "        rmse = np.sqrt(np.mean((sim-obs)**2))\n",
    "        bias = np.mean(sim-obs)\n",
    "        # kge\n",
    "        r = np.corrcoef(sim, obs)[0, 1]\n",
    "        alpha = np.std(sim) / np.std(obs)\n",
    "        beta = np.mean(sim) / np.mean(obs)\n",
    "        kge = 1 - np.sqrt((r - 1)**2 + (alpha - 1)**2 + (beta - 1)**2)\n",
    "        # r\n",
    "        pearson_r, _ = pearsonr(obs, sim)\n",
    "        # Timing error (lag of peak)\n",
    "        obs_peak = np.argmax(obs)\n",
    "        sim_peak = np.argmax(sim)\n",
    "        time_error = sim_peak - obs_peak\n",
    "        return rmse,bias,kge,time_error,pearson_r\n",
    "    else:\n",
    "        return np.nan,np.nan,np.nan,np.nan,np.nan\n",
    "\n",
    "def evaluate_fore(wlv_sim):\n",
    "    time_data = np.arange(0,23,dahour)\n",
    "    rmse_compare = np.full((np.shape(obs_point_ind)[0],int((time_len-tstart-24)/dahour),len(time_data)),np.nan)\n",
    "    bias_compare = np.full((np.shape(obs_point_ind)[0],int((time_len-tstart-24)/dahour),len(time_data)),np.nan)\n",
    "    kge_compare = np.full((np.shape(obs_point_ind)[0],int((time_len-tstart-24)/dahour),len(time_data)),np.nan)\n",
    "    time_error_compare = np.full((np.shape(obs_point_ind)[0],int((time_len-tstart-24)/dahour),len(time_data)),np.nan)\n",
    "    pearson_r_compare = np.full((np.shape(obs_point_ind)[0],int((time_len-tstart-24)/dahour),len(time_data)),np.nan)\n",
    "    test_time = 0\n",
    "    for time_ind in range(0,time_len-tstart-24,dahour):\n",
    "        for station_ind in range(0,np.shape(obs_point_ind)[0]):\n",
    "            if time_ind>24-1:\n",
    "                if 'ils01' in pm_path or 'round' in pm_path:\n",
    "                    continue\n",
    "                else:\n",
    "                    sim_data = np.nanmean(wlv_sim[:,tstart_da+int((time_ind-24)/dahour)+1:tstart_da+int(time_ind/dahour)+1,station_ind,:],axis=0)\n",
    "                    compare_lead_time = 23\n",
    "            else:\n",
    "                sim_data = np.nanmean(wlv_sim[:,tstart_da:tstart_da+int(time_ind/dahour)+1,station_ind,:],axis=0)\n",
    "                compare_lead_time = time_ind\n",
    "            for compare_ind in range(0,compare_lead_time,dahour):\n",
    "                # at least 1 hour ahead\n",
    "                if compare_lead_time-compare_ind+1>24:\n",
    "                    obs_len  = 24-(compare_lead_time-compare_ind-dahour)\n",
    "                    obs_data = obs_station[time_ind:time_ind+obs_len,station_ind]\n",
    "                    sim_each = sim_data[int(compare_ind/dahour),compare_lead_time-compare_ind-dahour:24]\n",
    "                else:\n",
    "                    obs_data = obs_station[time_ind:time_ind+dahour+1,station_ind]\n",
    "                    sim_each = sim_data[int(compare_ind/dahour),compare_lead_time-compare_ind-dahour:compare_lead_time-compare_ind+1]\n",
    "                rmse_compare[station_ind,int(time_ind/dahour),int(compare_ind/dahour)],bias_compare[station_ind,int(time_ind/dahour),int(compare_ind/dahour)],kge_compare[station_ind,int(time_ind/dahour),int(compare_ind/dahour)],time_error_compare[station_ind,int(time_ind/dahour),int(compare_ind/dahour)],pearson_r_compare[station_ind,int(time_ind/dahour),int(compare_ind/dahour)] = cal_single_metric(obs_data,sim_each)\n",
    "                test_time = test_time + 1\n",
    "                if np.shape(sim_each)[0]<2:\n",
    "                    print(time_ind,station_ind,compare_ind,compare_lead_time,obs_data,sim_each)\n",
    "                    exit()\n",
    "    # save leading time from 24 hours to 1 hour\n",
    "    rmse = np.nanmean(rmse_compare, axis=(1))\n",
    "    bias = np.nanmean(bias_compare, axis=(1))\n",
    "    kge  = np.nanmean(kge_compare, axis=(1))\n",
    "    time_error = np.nanmean(time_error_compare, axis=(1))\n",
    "    pearson_r = np.nanmean(pearson_r_compare, axis=(1))\n",
    "    # save leading time from 1 hour to 24 hours\n",
    "    return rmse[:,::-1],bias[:,::-1],kge[:,::-1]\n",
    "rmse_da,bias_da,kge_da = evaluate_fore(wlv_da)\n",
    "\n",
    "def cal_metric_sim():\n",
    "    metric_sim = np.full((np.shape(obs_point_ind)[0],3),np.nan)\n",
    "    for station_ind in range(0,np.shape(obs_point_ind)[0]):\n",
    "        obs_data = obs_point_ind[station_ind,:]\n",
    "        sim_data = np.nanmean(sim_point_ind[station_ind,:,:],axis=1)\n",
    "        metric_sim[station_ind,0],metric_sim[station_ind,1],metric_sim[station_ind,2],time_error,pearson_r = cal_single_metric(obs_data,sim_data)\n",
    "    return metric_sim\n",
    "# rmse,bias,kge\n",
    "metric_sim = cal_metric_sim()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c07679-2edd-4934-a0b5-c2c1e3c24a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate time error\n",
    "def evaluate_timeerror(wlv_da):\n",
    "    time_date = np.arange(0,23,dahour)\n",
    "    time_error_all = np.full((np.shape(obs_point_ind)[0],len(time_date)),np.nan) # from 1 to 24 hour leading time\n",
    "    time_error_sim = np.full((np.shape(obs_point_ind)[0]),np.nan) # from 1 to 24 hour leading time\n",
    "    for station_ind in range(0,np.shape(obs_point_ind)[0]):\n",
    "        obs_data = obs_point_ind[station_ind,:]\n",
    "        obs_peak = np.argmax(obs_data) + tstart\n",
    "        sim_mean = np.nanmean(sim_point_ind,axis=2)\n",
    "        sim_data = sim_mean[station_ind,:]\n",
    "        sim_peak = np.argmax(sim_data) + tstart\n",
    "        time_error_sim[station_ind]  = sim_peak-obs_peak\n",
    "        obs_peak_ind = int((obs_peak-1)/dahour)\n",
    "        obs_ind_remain = obs_peak-obs_peak_ind*dahour\n",
    "        if obs_peak_ind-tstart_da>int(24/dahour):\n",
    "            da_data_all = wlv_da[:,obs_peak_ind-int(24/dahour)+1:obs_peak_ind+1,:,:]\n",
    "            da_data = np.nanmean(da_data_all[:,:,station_ind,:],axis=0)\n",
    "            for time_ind in range(0,23,dahour):\n",
    "                sim_peak = np.argmax(da_data[int(time_ind/dahour),:])\n",
    "                # time_error_all[station_ind,int(time_ind/dahour)] = sim_peak-(23-time_ind) \n",
    "                time_error_all[station_ind,int(time_ind/dahour)] = sim_peak-(24-int(time_ind/dahour)*dahour-obs_ind_remain) \n",
    "                # <0 earlier then real\n",
    "                # >0 later then real\n",
    "    # [stations, leadtime]\n",
    "    time_error_mean = np.nanmean(time_error_all,axis=0)\n",
    "    return time_error_all[:,::-1],time_error_sim\n",
    "# [station,24hour]\n",
    "time_error_da,time_error_sim  = evaluate_timeerror(wlv_da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a05832-2114-4986-adad-d43d711df40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot bar error\n",
    "def plot_bar_error(ax,data_sim,data_da,ymin,ymax,ylabel):\n",
    "    def get_stats(data):\n",
    "        median = np.nanpercentile(data, 50, axis=0)\n",
    "        q25 = np.nanpercentile(data, 25, axis=0)\n",
    "        q75 = np.nanpercentile(data, 75, axis=0)\n",
    "        q10 = np.nanpercentile(data, 10, axis=0)\n",
    "        q90 = np.nanpercentile(data, 90, axis=0)\n",
    "        err_lower = median - q10\n",
    "        err_upper = q90 - median\n",
    "        return median,q25,q75,err_lower,err_upper\n",
    "    # sim\n",
    "    med1 = np.nanpercentile(data_sim, 50)\n",
    "    q25_1 = np.nanpercentile(data_sim, 25)\n",
    "    q75_1 = np.nanpercentile(data_sim, 75)\n",
    "    q10_1 = np.nanpercentile(data_sim, 10)\n",
    "    q90_1 = np.nanpercentile(data_sim, 90)\n",
    "    err1_low = med1 - q10_1\n",
    "    err1_high = q90_1 - med1\n",
    "    # DA\n",
    "    med2,q25_2,q75_2, err2_low, err2_high = get_stats(data_da)\n",
    "    x = np.arange(np.shape(data_da)[1]+1)\n",
    "    ax.axhline(y=0, color='gray', linestyle='--', linewidth=1.5)\n",
    "    bar_width = 0.4\n",
    "    ax.bar(x[0],q75_1-q25_1, bottom=q25_1, capsize=5, color='blue', width=bar_width, edgecolor='gray')\n",
    "    ax.bar(x[1:], q75_2-q25_2, bottom=q25_2, capsize=5, color='red', width=bar_width, edgecolor='gray')\n",
    "    for i in range(len(x)-1):\n",
    "        ax.plot(x[i+1], med2[i], marker='_', color='black', markersize=12, linewidth=150, zorder=3)\n",
    "    ax.plot(x[0], med1, marker='_', color='black', markersize=12, linewidth=150, zorder=3)\n",
    "    ax.set_xticks(range(np.shape(data_da)[1]+1))\n",
    "    if 'ils01' in pm_path:\n",
    "        ax.set_xticklabels(['sim']+[str(i) for i in range(1, np.shape(data_da)[1]+1)])\n",
    "    if 'ils03' in pm_path:\n",
    "        ax.set_xticklabels(['sim','1-3','4-6','7-9','10-12','13-15','16-18','19-21','22-24'])\n",
    "    if 'ils06' in pm_path:\n",
    "        ax.set_xticklabels(['sim','1-6','7-12','13-18','19-24'])\n",
    "    if 'ils12' in pm_path:\n",
    "        ax.set_xticklabels(['sim','1-12','13-24'])\n",
    "    ax.set_ylim(ymin,ymax)\n",
    "    ax.set_xlim(-0.5,np.shape(data_da)[1]+1-0.5)\n",
    "    ax.set_ylabel(ylabel,fontsize=12)\n",
    "    ax.set_xlabel('Hours after DA (h)',fontsize=12)\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 4),dpi=600)\n",
    "plot_bar_error(axs[0],metric_sim[:,0],rmse_da,0.6,2.4,'RMSE (m)')\n",
    "plot_bar_error(axs[1],metric_sim[:,1],bias_da,0,2.1,'Bias (m)')\n",
    "plot_bar_error(axs[2],time_error_sim,time_error_da,-5,8,'Timing error (h)')\n",
    "plt.tight_layout()\n",
    "plt.savefig(exp_plotdir+'4.metric.jpg', format='jpg',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c15dae1-5255-41f3-8024-22c39436babe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% evaluate the 24 hours forecast performance which is ahead of peak flow\n",
    "def evaluate_24h(wlv_da,sim_point_ind,obs_point_ind):\n",
    "    # [stations,ahead_hours,five_vars]\n",
    "    da_matrix  = np.full((np.shape(obs_point_ind)[0],int(24/dahour),5),np.nan) # from 1 to 24 hours ahead\n",
    "    sim_matrix = np.full((np.shape(obs_point_ind)[0],int(24/dahour),5),np.nan) # from 1 to 24 hours ahead\n",
    "    for station_ind in range(0,np.shape(obs_point_ind)[0]):\n",
    "        obs_data = obs_point_ind[station_ind,:]\n",
    "        obs_peak_ind = np.argmax(obs_data) + tstart_da\n",
    "        obs_peak = np.argmax(obs_data) + tstart\n",
    "        obs_peak1 = math.ceil((obs_peak-24)/dahour)\n",
    "        obs_peak2 = math.floor(obs_peak/dahour)\n",
    "        da_data  = np.nanmean(wlv_da[:,obs_peak1:obs_peak2,station_ind,:],axis=0)\n",
    "        # 1-24 hours ahead\n",
    "        if obs_peak1-tstart_da<0:\n",
    "            continue\n",
    "        else:\n",
    "            for t_ahead in range(1,1+int(24/dahour)):\n",
    "                sim_use = np.nanmean(sim_point_ind[station_ind,obs_peak-tstart-t_ahead:obs_peak-tstart-t_ahead+24,:],axis=1)\n",
    "                obs_use = obs_point_ind[station_ind,obs_peak-tstart-t_ahead:obs_peak-tstart-t_ahead+24]\n",
    "                da_use  = da_data[obs_peak2-obs_peak1-t_ahead,:]\n",
    "                # if (station_ind == 7) & ((t_ahead == 1) | (t_ahead == 5)):\n",
    "                #     print(t_ahead,sim_use,obs_use,da_use)\n",
    "                da_matrix[station_ind,t_ahead-1,0],da_matrix[station_ind,t_ahead-1,1],da_matrix[station_ind,t_ahead-1,2],da_matrix[station_ind,t_ahead-1,3],da_matrix[station_ind,t_ahead-1,4]  = cal_single_metric(obs_use,da_use)\n",
    "                sim_matrix[station_ind,t_ahead-1,0],sim_matrix[station_ind,t_ahead-1,1],sim_matrix[station_ind,t_ahead-1,2],sim_matrix[station_ind,t_ahead-1,3],sim_matrix[station_ind,t_ahead-1,4] = cal_single_metric(obs_use,sim_use)\n",
    "    # rmse,bias,kge,time_error,pearson_r\n",
    "    return da_matrix,sim_matrix\n",
    "da_matrix_wlv,sim_matrix_wlv = evaluate_24h(wlv_da,sim_point_ind,obs_point_ind)\n",
    "da_matrix_dis,sim_matrix_dis = evaluate_24h(dis_da_use,sim_point_dis,obs_point_dis)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffc14f1-1a39-4d83-b020-1a69b8c15062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot bar evaluation for 24 hours forecast\n",
    "def plot_bar_24fore(ax,data_sim,data_da,ymin,ymax,ylabel):\n",
    "    def get_stats(data):\n",
    "        median = np.nanpercentile(data, 50, axis=0)\n",
    "        q25 = np.nanpercentile(data, 25, axis=0)\n",
    "        q75 = np.nanpercentile(data, 75, axis=0)\n",
    "        q10 = np.nanpercentile(data, 10, axis=0)\n",
    "        q90 = np.nanpercentile(data, 90, axis=0)\n",
    "        err_lower = median - q10\n",
    "        err_upper = q90 - median\n",
    "        return median,q25,q75,err_lower,err_upper\n",
    "    # sim\n",
    "    med1,q25_1,q75_1, err1_low, err1_high = get_stats(data_sim)\n",
    "    # DA\n",
    "    med2,q25_2,q75_2, err2_low, err2_high = get_stats(data_da)\n",
    "    x = np.arange(np.shape(data_da)[1])\n",
    "    ax.axhline(y=0, color='gray', linestyle='--', linewidth=1.5)\n",
    "    bar_width = 0.4\n",
    "    ax.bar(x-bar_width/2,q75_1-q25_1, bottom=q25_1, capsize=5, color='blue', width=bar_width, edgecolor='gray')\n",
    "    ax.bar(x+bar_width/2, q75_2-q25_2, bottom=q25_2, capsize=5, color='red', width=bar_width, edgecolor='gray')\n",
    "    for i in range(len(x)):\n",
    "        ax.plot(x[i]-bar_width/2, med1[i], marker='_', color='black', markersize=12, linewidth=150, zorder=3)\n",
    "        ax.plot(x[i]+bar_width/2, med2[i], marker='_', color='black', markersize=12, linewidth=150, zorder=3)\n",
    "    ax.set_xticks(np.arange(0,np.shape(data_da)[1]))\n",
    "    if 'ils01' in pm_path or 'round' in pm_path:\n",
    "        ax.set_xticklabels([str(i) for i in range(1, 25)])\n",
    "    if 'ils03' in pm_path:\n",
    "        ax.set_xticklabels(['1-3','4-6','7-9','10-12','13-15','16-18','19-21','22-24'])\n",
    "    if 'ils06' in pm_path:\n",
    "        ax.set_xticklabels(['1-6','7-12','13-18','19-24'])\n",
    "    if 'ils12' in pm_path:\n",
    "        ax.set_xticklabels(['1-12','13-24'])\n",
    "    ax.set_ylim(ymin,ymax)\n",
    "    ax.set_xlim(-0.5,np.nanmax(x)+0.5)\n",
    "    ax.set_ylabel(ylabel,fontsize=12)\n",
    "    ax.set_xlabel('Forecast Leading Time Before Peak Flow (h)',fontsize=12)\n",
    "\n",
    "def plot_24matirx(sim_matrix_wlv,da_matrix_wlv,y1,y2,y3,y4,y5,y6,y7,y8,varname):\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(16, 8),dpi=600)\n",
    "    if varname == 'wlv':\n",
    "        plot_bar_24fore(axs[0,0],sim_matrix_wlv[:,:,0],da_matrix_wlv[:,:,0],y1,y2,'RMSE (m)')\n",
    "        plot_bar_24fore(axs[0,1],sim_matrix_wlv[:,:,1],da_matrix_wlv[:,:,1],y3,y4,'Bias (m)')\n",
    "    else:\n",
    "        plot_bar_24fore(axs[0,0],sim_matrix_wlv[:,:,0],da_matrix_wlv[:,:,0],y1,y2,'RMSE (m${^3}$/s)')\n",
    "        plot_bar_24fore(axs[0,1],sim_matrix_wlv[:,:,1],da_matrix_wlv[:,:,1],y3,y4,'Bias (m${^3}$/s)')\n",
    "    plot_bar_24fore(axs[1,0],sim_matrix_wlv[:,:,2],da_matrix_wlv[:,:,2],y5,y6,'KGE')\n",
    "    plot_bar_24fore(axs[1,1],sim_matrix_wlv[:,:,4],da_matrix_wlv[:,:,4],y7,y8,'Pearson Correlation Coefficient')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(exp_plotdir+'5.24hour_metric_'+varname+'.jpg', format='jpg',dpi=600)\n",
    "    plt.show()\n",
    "if 'ils01' in pm_path or 'round' or 'msm' in pm_path:\n",
    "    plot_24matirx(sim_matrix_wlv,da_matrix_wlv,0.5,4.,-1.6,3.8,-5,0.8,-0.,1,'wlv')\n",
    "    plot_24matirx(sim_matrix_dis,da_matrix_dis,80,2600,-1100,2200,-7.2,0.8,-0.,1,'dis')\n",
    "if 'ils03' in pm_path:\n",
    "    plot_24matirx(sim_matrix_wlv,da_matrix_wlv,0.3,3.5,-0.6,3.2,-3,0.8,-0.8,1,'wlv')\n",
    "    plot_24matirx(sim_matrix_dis,da_matrix_dis,0,750,-250,500,-3,0.8,-0.8,1,'dis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cbb229-1a65-4788-bef8-36fdc2693f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% plot flood extend\n",
    "filepath = '/data50/yingying/flood/'\n",
    "def read_cov_tif(filename):\n",
    "    filen = filepath + filename\n",
    "    with rasterio.open(filen) as dataset:\n",
    "        tiff_data = dataset.read(1)\n",
    "        # tiff_data = tiff_data[::-1,:]\n",
    "        tiff_data = np.where(tiff_data==0,np.nan,tiff_data)\n",
    "        col = dataset.width      \n",
    "        row = dataset.height     \n",
    "        bounds = dataset.bounds\n",
    "        min_lon, min_lat, max_lon, max_lat = bounds\n",
    "        print(f\"Longitude Range: {min_lon} to {max_lon}\")\n",
    "        print(f\"Latitude Range: {min_lat} to {max_lat}\")\n",
    "        print('--------')\n",
    "    return col,row,min_lon,max_lon,min_lat,max_lat,tiff_data\n",
    "# 20191012T2042 UST\n",
    "col1,row1,min_lon1,max_lon1,min_lat1,max_lat1,fld_reg1 = read_cov_tif('area_1_result.tif')  # 138-138.4, 36.6-36.8\n",
    "col2,row2,min_lon2,max_lon2,min_lat2,max_lat2,fld_reg2 = read_cov_tif('area_2_result.tif')  # 139.3-139.8, 34.45-35.7\n",
    "\n",
    "# col1,row1,min_lon1,max_lon1,min_lat1,max_lat1,fld_reg1 = read_cov_tif('binary_output_without_water_area2.tif')  # 137.4-140.6, 35.9-37.8\n",
    "# col2,row2,min_lon2,max_lon2,min_lat2,max_lat2,fld_reg2 = read_cov_tif('binary_output_without_water_coast.tif')  # 138.1-141.2, 34.5-36.4\n",
    "\n",
    "lon_map1 = np.linspace(min_lon1,max_lon1,col1)\n",
    "lat_map1 = np.linspace(min_lat1,max_lat1,row1)\n",
    "lon2d_map1, lat2d_map1 = np.meshgrid(lon_map1, lat_map1)\n",
    "lon_map2 = np.linspace(min_lon2,max_lon2,col2)\n",
    "lat_map2 = np.linspace(min_lat2,max_lat2,row2)\n",
    "lon2d_map2, lat2d_map2 = np.meshgrid(lon_map2, lat_map2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4759b38-b4b0-4cc3-b44e-3e18b38d0963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure6: SAR map\n",
    "font_label = {'family': 'serif', 'weight': 'normal', 'size': 10.5}\n",
    "def draw_sat_fld():\n",
    "    fig = plt.figure(dpi = 600,figsize=(6,6))\n",
    "    ax1 = fig.add_axes([0.1,0.1,0.8,0.8],projection=ccrs.PlateCarree())\n",
    "    ax1.set_axis_off()\n",
    "    ax1.set_extent([lon_min,lon_max,lat_min,lat_max], ccrs.PlateCarree())\n",
    "    gl = ax1.gridlines(crs=ccrs.PlateCarree(), draw_labels=True,\n",
    "                    linewidth=1, color='gray', alpha=0, linestyle='-.')\n",
    "    ax1.coastlines(alpha=1.,linestyle='-',color = \"#3F3A3A\",lw=0.8,resolution='10m')\n",
    "    # mask the other place\n",
    "    region_lon1 = [128.06, 128.06,138, 138 ]\n",
    "    region_lat1 = [41 , 45.95 , 45.95, 41  ]\n",
    "    region_lon2 = [128.06,128.06,131.4,131.4 ]\n",
    "    region_lat2 = [34.5,41,41,34.5]\n",
    "    ax1.fill(region_lon1, region_lat1, color='white', transform=ccrs.PlateCarree(), zorder=10)\n",
    "    ax1.fill(region_lon2, region_lat2, color='white', transform=ccrs.PlateCarree(), zorder=10)\n",
    "    cmap = ListedColormap(['blue']) \n",
    "    gl.top_labels   = False\n",
    "    gl.right_labels = False\n",
    "    gl.bottom_labels   = False\n",
    "    gl.left_labels = False\n",
    "    lon2d_tiff, lat2d_tiff = np.meshgrid(lon_tiff, lat_tiff)\n",
    "    uparea_reg = np.where(np.isnan(uparea_low),np.nan,0.002)\n",
    "    ax1.scatter(lon2d_tiff, lat2d_tiff,uparea_reg, c='blue',zorder=2, transform=ccrs.PlateCarree())\n",
    "    # con_plot1 = ax1.pcolormesh(lon_tiff,lat_tiff,uparea_low,zorder=2, cmap=cmap,alpha=0.35, transform=ccrs.PlateCarree(),vmin=0,vmax=30)  \n",
    "    def draw_sat_reg(lon2d_map,lat2d_map,data):\n",
    "        # control the size for the flooding area\n",
    "        ax1.scatter(lon2d_map, lat2d_map, data, color='black', zorder=3, transform=ccrs.PlateCarree())\n",
    "    draw_sat_reg(lon2d_map1,lat2d_map1,fld_reg1[::-1,:])\n",
    "    draw_sat_reg(lon2d_map2,lat2d_map2,fld_reg2[::-1,:])\n",
    "    plt.savefig(inputdir+'6_SARmap.jpg', format='jpg',dpi=600)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "# draw_sat_fld()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf09bb6-61db-4a66-9038-3e0a99e64324",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%% read flood extend data\n",
    "# read sim/DA output\n",
    "def read_flood(dhour):\n",
    "    da_data = []\n",
    "    sim_data = []\n",
    "    SAR_time = datetime(2019,10,12,21)\n",
    "    time_step = timedelta(hours=dhour)\n",
    "    ctime = SAR_time - time_step\n",
    "    cyyyy='%04d' % (ctime.year)\n",
    "    cmm='%02d' % (ctime.month)\n",
    "    cdd='%02d' % (ctime.day)\n",
    "    chh='%02d' % (ctime.hour)\n",
    "    ctime = cyyyy + cmm + cdd + chh\n",
    "    for ens in range(0,pm.ens_mem()):\n",
    "        ens_str = '{:03d}'.format(ens+1)\n",
    "        if os.path.exists(outdir+'/CaMa_out_'+expname+dahour_str+'/'+ctime+'A'+ens_str+'/o_fldfrc'+syyyy+'.nc'):\n",
    "            fname_da  = outdir+'/CaMa_out_'+expname+dahour_str+'/'+ctime+'A'+ens_str+'/o_fldfrc'+syyyy+'.nc'\n",
    "            fname_sim = outdir+'/CaMa_out_'+expname+dahour_str+'/'+ctime+'C'+ens_str+'/o_fldfrc'+syyyy+'.nc'\n",
    "        else:\n",
    "            chour = math.floor(dahour)\n",
    "            chh = '{:02d}'.format(chour)\n",
    "            ctime = cyyyy + cmm + cdd + chh\n",
    "            fname_da  = outdir+'/CaMa_out_'+expname+dahour_str+'/'+ctime+'A'+ens_str+'/o_fldfrc'+syyyy+'.nc'\n",
    "            fname_sim = outdir+'/CaMa_out_'+expname+dahour_str+'/'+ctime+'C'+ens_str+'/o_fldfrc'+syyyy+'.nc'\n",
    "        flood_da  = read_nc(fname_da,3)\n",
    "        flood_sim = read_nc(fname_sim,3)\n",
    "        lon_region_ind = np.where((lon>=lon_min)&(lon<=lon_max))[0]\n",
    "        lon_region = lon[lon_region_ind]\n",
    "        lat_region_ind = np.where((lat>=lat_min)&(lat<=lat_max))[0]\n",
    "        lat_region = lat[lat_region_ind]\n",
    "        flood_da_region  = flood_da[dhour,lat_region_ind[:,None],lon_region_ind]\n",
    "        flood_sim_region = flood_sim[dhour,lat_region_ind[:,None],lon_region_ind]\n",
    "        da_data.append(flood_da_region)\n",
    "        sim_data.append(flood_sim_region)\n",
    "        break\n",
    "    # da_all = np.nanmean(np.stack(da_data,axis=0),axis=0)\n",
    "    # sim_all = np.nanmean(np.stack(sim_data,axis=0),axis=0)\n",
    "    # print(np.shape(da_all)) # [180,240]\n",
    "    return lon_region,lat_region\n",
    "    \n",
    "# [lat,lon,time_ahead]\n",
    "lon_region,lat_region = read_flood(21)\n",
    "# flood_da = np.full((len(lat_region),len(lon_region),23),np.nan)\n",
    "# flood_sim = np.full((len(lat_region),len(lon_region),23),np.nan)\n",
    "# for dhour in range(1,24):\n",
    "#     flood_da_each,flood_sim_each,lon_region,lat_region = read_flood(dhour)\n",
    "#     flood_da[:,:,dhour-1] = flood_da_each\n",
    "#     flood_sim[:,:,dhour-1] = flood_sim_each\n",
    "    \n",
    "\n",
    "filepath = inputdir+'/exp_'+expname+dahour_str+'/'\n",
    "# np.save(filepath+'flood'+'/flood_da.npy',flood_da)\n",
    "# np.save(filepath+'flood'+'/flood_sim.npy',flood_sim)\n",
    "flood_da = np.load(filepath+'flood'+'/flood_da.npy')\n",
    "flood_sim = np.load(filepath+'flood'+'/flood_sim.npy')\n",
    "\n",
    "def find_sim_region(min_lon,max_lon,min_lat,max_lat):\n",
    "    lat_ind = np.where((lat_region>=min_lat)&(lat_region<=max_lat))[0]\n",
    "    lon_ind = np.where((lon_region>=min_lon)&(lon_region<=max_lon))[0]\n",
    "    lat_reg = lat_region[np.nanmin(lat_ind):np.nanmax(lat_ind)+1]\n",
    "    lon_reg = lon_region[np.nanmin(lon_ind):np.nanmax(lon_ind)+1]\n",
    "    lon2d_map, lat2d_map = np.meshgrid(lon_reg, lat_reg)\n",
    "    sim_fld_reg = flood_sim[np.nanmin(lat_ind):np.nanmax(lat_ind)+1,np.nanmin(lon_ind):np.nanmax(lon_ind)+1,:]\n",
    "    da_fld_reg  = flood_da[np.nanmin(lat_ind):np.nanmax(lat_ind)+1,np.nanmin(lon_ind):np.nanmax(lon_ind)+1,:]\n",
    "    return lon2d_map, lat2d_map,sim_fld_reg,da_fld_reg\n",
    "lon2d_sim1,lat2d_sim1,sim_fld_reg1,da_fld_reg1 = find_sim_region(min_lon1,max_lon1,min_lat1,max_lat1)\n",
    "lon2d_sim2,lat2d_sim2,sim_fld_reg2,da_fld_reg2 = find_sim_region(min_lon2,max_lon2,min_lat2,max_lat2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d55cc1-d4b2-45ea-ba51-0d0d1d6e8864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upscaling for SAR data\n",
    "window_size = 46   #93 grids, depends on resolution\n",
    "def upscale_sat(fld_reg,sim_fld_reg,da_fld_reg,exp_type):\n",
    "    fld_upscale = np.full((np.shape(sim_fld_reg)[0],np.shape(sim_fld_reg)[1]),np.nan)\n",
    "    for i in range(0,np.shape(sim_fld_reg)[0]): # lat\n",
    "        for j in range(0,np.shape(sim_fld_reg)[1]): # lon\n",
    "            sim = sim_fld_reg[i,j]\n",
    "            da  = da_fld_reg[i,j]\n",
    "            lat_i1 = i*window_size*2-window_size\n",
    "            lat_i2 = i*window_size*2+window_size\n",
    "            lon_i1 = j*window_size*2-window_size\n",
    "            lon_i2 = j*window_size*2+window_size\n",
    "            if lat_i1<0:\n",
    "                lat_i1 = 0\n",
    "            if lon_i1<0:\n",
    "                lon_i1 = 0            \n",
    "            if lat_i2>np.shape(fld_reg)[0]:\n",
    "                lat_i2 = np.shape(fld_reg)[0]    \n",
    "            if lon_i2>np.shape(fld_reg)[1]:\n",
    "                lon_i2 = np.shape(fld_reg)[1]\n",
    "            # print(lat_i1,lat_i2,lon_i1,lon_i2]))\n",
    "            fld_data = fld_reg[lat_i1:lat_i2,lon_i1:lon_i2]\n",
    "            if exp_type == 'weight':\n",
    "                fld_upscale[i,j] = np.nansum(fld_data)/((lat_i2-lat_i1)*(lon_i2-lon_i1))\n",
    "            else:\n",
    "                if np.nansum(fld_data)>0:\n",
    "                    fld_upscale[i,j] = 1\n",
    "                else:\n",
    "                    fld_upscale[i,j] = 0\n",
    "    return fld_upscale\n",
    "fld_up1 = upscale_sat(fld_reg1,sim_fld_reg1,da_fld_reg1,'weight')\n",
    "fld_up2 = upscale_sat(fld_reg2,sim_fld_reg2,da_fld_reg2,'weight')\n",
    "fld_grid1 = upscale_sat(fld_reg2,sim_fld_reg2,da_fld_reg2,'grid')\n",
    "fld_grid2 = upscale_sat(fld_reg2,sim_fld_reg2,da_fld_reg2,'grid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2374e70-c6d8-4cdb-88ad-868de6698742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure6: sim flooding map\n",
    "lon_ret1, lon_ret2,lat_ret1, lat_ret2 = 138,138.41,36.6,36.8\n",
    "lon_ret3, lon_ret4,lat_ret3, lat_ret4 = 139.28,139.83,35.44,35.72\n",
    "# lon_ret3, lon_ret4,lat_ret3, lat_ret4 = 139.4,140,35.5,36.3\n",
    "def draw_sat_fld():\n",
    "    fig = plt.figure(dpi = 600,figsize=(6,6))\n",
    "    ax1 = fig.add_axes([0.1,0.1,0.8,0.8],projection=ccrs.PlateCarree())\n",
    "    ax1.set_axis_off()\n",
    "    ax1.set_extent([136.5,141,34,37], ccrs.PlateCarree())\n",
    "    gl = ax1.gridlines(crs=ccrs.PlateCarree(), draw_labels=True,\n",
    "                    linewidth=1, color='gray', alpha=0, linestyle='-.')\n",
    "    ax1.coastlines(alpha=1.,linestyle='-',color = \"#3F3A3A\",lw=0.8,resolution='10m')\n",
    "    # mask the other place\n",
    "    region_lon1 = [128.06, 128.06,138, 138 ]\n",
    "    region_lat1 = [41 , 45.95 , 45.95, 41  ]\n",
    "    region_lon2 = [128.06,128.06,131.4,131.4 ]\n",
    "    region_lat2 = [34.5,41,41,34.5]\n",
    "    ax1.fill(region_lon1, region_lat1, color='white', transform=ccrs.PlateCarree(), zorder=10)\n",
    "    ax1.fill(region_lon2, region_lat2, color='white', transform=ccrs.PlateCarree(), zorder=10)\n",
    "    cmap = ListedColormap(['blue']) \n",
    "    gl.top_labels   = False\n",
    "    gl.right_labels = False\n",
    "    # identify region\n",
    "    # 138-138.4, 36.6-36.8\n",
    "    # 139.3-139.8, 35.45-35.7\n",
    "    def add_rect(lon_min, lon_max,lat_min,lat_max):\n",
    "        rect = Rectangle((lon_min,lat_min), lon_max-lon_min, lat_max-lat_min, linewidth=0.5, edgecolor='black',facecolor='none',transform=ccrs.PlateCarree())\n",
    "        ax1.add_patch(rect)\n",
    "    add_rect(lon_ret1, lon_ret2,lat_ret1, lat_ret2)\n",
    "    add_rect(lon_ret3, lon_ret4,lat_ret3, lat_ret4)\n",
    "    lon2d_tiff, lat2d_tiff = np.meshgrid(lon_tiff, lat_tiff)\n",
    "    uparea_reg = np.where(np.isnan(uparea_low),np.nan,0.01)\n",
    "    ax1.scatter(lon2d_tiff, lat2d_tiff,uparea_reg, c='blue',zorder=2, transform=ccrs.PlateCarree())\n",
    "    # con_plot1 = ax1.pcolormesh(lon_tiff,lat_tiff,uparea_low,zorder=2, cmap=cmap,alpha=0.35, transform=ccrs.PlateCarree(),vmin=0,vmax=30)  \n",
    "    fld_up_plot1 = np.where(np.isnan(fld_up1)|(fld_up1<10**(-8)),np.nan,0.5)\n",
    "    fld_up_plot2 = np.where(np.isnan(fld_up2)|(fld_up2<10**(-8)),np.nan,0.5)\n",
    "    def draw_sat_reg(lon2d_map,lat2d_map,data,c):\n",
    "        # control the size for the flooding area\n",
    "        ax1.scatter(lon2d_map, lat2d_map, data, color=c, zorder=3, transform=ccrs.PlateCarree(),marker='.')\n",
    "    draw_sat_reg(lon2d_sim1,lat2d_sim1,sim_fld_reg1[:,:,2],'orange')\n",
    "    draw_sat_reg(lon2d_sim1,lat2d_sim1,da_fld_reg1[:,:,2],'red')\n",
    "    draw_sat_reg(lon2d_sim2,lat2d_sim2,sim_fld_reg2[:,:,2],'orange')\n",
    "    draw_sat_reg(lon2d_sim2,lat2d_sim2,da_fld_reg2[:,:,2],'red')\n",
    "    draw_sat_reg(lon2d_sim1,lat2d_sim1,fld_up_plot1,'black')\n",
    "    draw_sat_reg(lon2d_sim2,lat2d_sim2,fld_up_plot2,'black')\n",
    "    # draw_sat_reg(lon2d_map1,lat2d_map1,fld_reg1[::-1,:]*0.05,'black')\n",
    "    # draw_sat_reg(lon2d_map2,lat2d_map2,fld_reg2[::-1,:]*0.05,'black')\n",
    "    plt.savefig(exp_plotdir+'6.24hour_flood_map.jpg', format='jpg',dpi=600)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "draw_sat_fld()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdf4bc0-9b8c-408c-ae3b-e304dea9e270",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# region compare\n",
    "def cal_comp_fld(fld_up,sim_fld_reg,da_fld_reg):\n",
    "    cal_sim_fld = np.zeros((np.shape(sim_fld_reg)[0],np.shape(sim_fld_reg)[1]))\n",
    "    cal_da_fld  = np.zeros((np.shape(sim_fld_reg)[0],np.shape(sim_fld_reg)[1]))\n",
    "    cal_fld     = np.full((np.shape(sim_fld_reg)[0],np.shape(sim_fld_reg)[1]),np.nan)\n",
    "    size_fld    = np.full((np.shape(sim_fld_reg)[0],np.shape(sim_fld_reg)[1]),np.nan)\n",
    "    for i in range(0,np.shape(sim_fld_reg)[0]): # lat\n",
    "        for j in range(0,np.shape(sim_fld_reg)[1]): # lon\n",
    "            # simulation condition\n",
    "            if (fld_up[i,j]<10**(-8)) & (sim_fld_reg[i,j]>10**(-8)):  # only sim\n",
    "                cal_sim_fld[i,j] = 1\n",
    "            if (fld_up[i,j]>10**(-8)) & (sim_fld_reg[i,j]>10**(-8)):  # both sim and sat\n",
    "                cal_sim_fld[i,j] = 2\n",
    "            if (fld_up[i,j]>10**(-8)) & (sim_fld_reg[i,j]<10**(-8)):  # only sat\n",
    "                cal_sim_fld[i,j] = 3\n",
    "            # da condition\n",
    "            if (fld_up[i,j]<10**(-8)) & (da_fld_reg[i,j]>10**(-8)):  # only da\n",
    "                cal_da_fld[i,j] = 1\n",
    "            if (fld_up[i,j]>10**(-8)) & (da_fld_reg[i,j]>10**(-8)):  # both da and sat\n",
    "               cal_da_fld[i,j] = 2\n",
    "            if (fld_up[i,j]>10**(-8)) & (da_fld_reg[i,j]<10**(-8)):  # only sat\n",
    "                cal_da_fld[i,j] = 3\n",
    "            # 0.5: only sim, 1.5: only da, 2.5: only sat, 3.5: sim+da, 4.5: sim+sat, 5.5: da+sat, 6.5: sim+da+sat, nan: no flood\n",
    "            if (cal_sim_fld[i,j]==1) & (cal_da_fld[i,j]==0):\n",
    "                cal_fld[i,j] = 0.5\n",
    "                size_fld[i,j]= sim_fld_reg[i,j]*100\n",
    "            if (cal_sim_fld[i,j]==0) & (cal_da_fld[i,j]==1):\n",
    "                cal_fld[i,j] = 1.5\n",
    "                size_fld[i,j]= da_fld_reg[i,j]*100\n",
    "            if (cal_sim_fld[i,j]==3) & (cal_da_fld[i,j]==3):\n",
    "                cal_fld[i,j] = 2.5                \n",
    "                size_fld[i,j]= fld_up[i,j]*100\n",
    "            if (cal_sim_fld[i,j]==1) & (cal_da_fld[i,j]==1):\n",
    "                cal_fld[i,j] = 3.5            \n",
    "                size_fld[i,j]= sim_fld_reg[i,j]*100\n",
    "            if (cal_sim_fld[i,j]==2) & (cal_da_fld[i,j]==3):\n",
    "                cal_fld[i,j] = 4.5                \n",
    "                size_fld[i,j]= fld_up[i,j]*100\n",
    "            if (cal_sim_fld[i,j]==3) & (cal_da_fld[i,j]==2):\n",
    "                cal_fld[i,j] = 5.5\n",
    "                size_fld[i,j]= fld_up[i,j]*100\n",
    "            if (cal_sim_fld[i,j]==2) & (cal_da_fld[i,j]==2):\n",
    "                cal_fld[i,j] = 6.5      \n",
    "                size_fld[i,j]= fld_up[i,j]*100\n",
    "    def cal_csi(cal_sim_fld,sim_fld_reg):\n",
    "        sim_bottom = 0\n",
    "        sim_up = 0\n",
    "        for i in range(0,np.shape(sim_fld_reg)[0]): # lat\n",
    "            for j in range(0,np.shape(sim_fld_reg)[1]): # lon\n",
    "                if cal_sim_fld[i,j] == 1:\n",
    "                    sim_bottom = sim_bottom + sim_fld_reg[i,j]\n",
    "                if cal_sim_fld[i,j] == 2:\n",
    "                    sim_up = sim_up + fld_up[i,j]\n",
    "                    sim_bottom = sim_bottom + np.abs(sim_fld_reg[i,j]-fld_up[i,j])\n",
    "                    # print(sim_fld_reg[i,j],fld_up[i,j],sim_fld_reg[i,j])\n",
    "                if cal_sim_fld[i,j] == 3:\n",
    "                    sim_bottom = sim_bottom + fld_up[i,j]\n",
    "        return sim_up/sim_bottom\n",
    "    csi_sim = cal_csi(cal_sim_fld,sim_fld_reg)\n",
    "    csi_da  = cal_csi(cal_da_fld,da_fld_reg)\n",
    "    print('score sim:','{:.2f}'.format(csi_sim),'score_da:','{:.2f}'.format(csi_da))\n",
    "    return cal_fld,size_fld,cal_sim_fld,cal_da_fld,csi_sim,csi_da\n",
    "\n",
    "# from forecast time ahead 1hour to 24hours\n",
    "def evaluate_flood(fld_up,sim_fld_reg,da_fld_reg):\n",
    "    cal_sim_fld = np.zeros((np.shape(sim_fld_reg)[0],np.shape(sim_fld_reg)[1],np.shape(sim_fld_reg)[2]))\n",
    "    cal_da_fld  = np.zeros((np.shape(sim_fld_reg)[0],np.shape(sim_fld_reg)[1],np.shape(sim_fld_reg)[2]))\n",
    "    cal_fld     = np.full((np.shape(sim_fld_reg)[0],np.shape(sim_fld_reg)[1],np.shape(sim_fld_reg)[2]),np.nan)\n",
    "    size_fld    = np.full((np.shape(sim_fld_reg)[0],np.shape(sim_fld_reg)[1],np.shape(sim_fld_reg)[2]),np.nan)\n",
    "    csi_sim = np.full(np.shape(sim_fld_reg)[2],np.nan)\n",
    "    csi_da  = np.full(np.shape(sim_fld_reg)[2],np.nan)\n",
    "    for i in range(0,np.shape(sim_fld_reg)[2]):\n",
    "        cal_fld[:,:,i],size_fld[:,:,i],cal_sim_fld[:,:,i],cal_da_fld[:,:,i],csi_sim[i],csi_da[i] = cal_comp_fld(fld_up,sim_fld_reg[:,:,i],da_fld_reg[:,:,i])\n",
    "    return cal_fld,size_fld,cal_sim_fld,cal_da_fld,csi_sim,csi_da\n",
    "cal_fld1,size_fld1,cal_sim_fld1,cal_da_fld1,csi_sim1,csi_da1 = evaluate_flood(fld_up1,sim_fld_reg1,da_fld_reg1)\n",
    "cal_fld2,size_fld2,cal_sim_fld2,cal_da_fld2,csi_sim2,csi_da2 = evaluate_flood(fld_up2,sim_fld_reg2,da_fld_reg2)\n",
    "print('reg1:',np.argmax(csi_da1-csi_sim1))\n",
    "print('reg2:',np.argmax(csi_da2-csi_sim2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5839de46-56ab-4aa7-8b29-802a7c4f05f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_flood_condition(csi_da1,csi_sim1,csi_da2,csi_sim2):\n",
    "    fig = plt.figure(dpi = 600,figsize=(6,6))\n",
    "    ax1 = fig.add_axes([0.1,0.65,0.7,0.35],projection=ccrs.PlateCarree())\n",
    "    ax2 = fig.add_axes([0.1,0.26,0.7,0.38],projection=ccrs.PlateCarree())\n",
    "    ax3 = fig.add_axes([0.1,0.1,0.7,0.15])\n",
    "    def comp_fld(ax1, lon_ret1, lon_ret2,lat_ret1, lat_ret2, lon2d_sim, lat2d_sim, cal_fld,size_fld):\n",
    "        ax1.set_extent([lon_ret1, lon_ret2,lat_ret1, lat_ret2], ccrs.PlateCarree())\n",
    "        gl = ax1.gridlines(crs=ccrs.PlateCarree(), draw_labels=True,\n",
    "                        linewidth=1, color='gray', alpha=0, linestyle='-.')\n",
    "        ax1.coastlines(alpha=1.,linestyle='-',color = \"#3F3A3A\",lw=0.8,resolution='10m')\n",
    "        cmap_blue = ListedColormap(['blue']) \n",
    "        colors = plt.cm.rainbow(np.linspace(0, 1, 7)) \n",
    "        cmap = ListedColormap(colors) \n",
    "        bounds = np.arange(0, 8)\n",
    "        norm = BoundaryNorm(bounds, cmap.N) \n",
    "        gl.top_labels   = False\n",
    "        gl.right_labels = False\n",
    "        gl.bottom_labels= False\n",
    "        gl.left_labels = False\n",
    "        lon_map = np.linspace(lon_min,lon_max,int((lon_max-lon_min)*3600))\n",
    "        lat_map = np.linspace(lat_min,lat_max,int((lat_max-lat_min)*3600))\n",
    "        lon_tiff_ind = np.where((lon_map>=lon_ret1)&(lon_map<=lon_ret2))[0]\n",
    "        lat_tiff_ind = np.where((lat_map>=lat_ret1)&(lat_map<=lat_ret2))[0]\n",
    "        lat_tiff_reg = lat_map[np.nanmin(lat_tiff_ind):np.nanmax(lat_tiff_ind)+1]\n",
    "        lon_tiff_reg = lon_map[np.nanmin(lon_tiff_ind):np.nanmax(lon_tiff_ind)+1]\n",
    "        lon2d_tiff, lat2d_tiff = np.meshgrid(lon_tiff_reg, lat_tiff_reg)\n",
    "        uparea_reg = up_area[np.nanmin(lat_tiff_ind):np.nanmax(lat_tiff_ind)+1,np.nanmin(lon_tiff_ind):np.nanmax(lon_tiff_ind)+1]\n",
    "        uparea_reg = np.where(uparea_reg<10,np.nan,0.01)\n",
    "        ax1.scatter(lon2d_tiff, lat2d_tiff,uparea_reg, c='blue',zorder=3, transform=ccrs.PlateCarree())\n",
    "        ## ax1.pcolormesh(lon_tiff_reg,lat_tiff_reg,uparea_reg,zorder=2, cmap=cmap_blue, transform=ccrs.PlateCarree())  \n",
    "        con_plot2 = ax1.scatter(lon2d_sim, lat2d_sim, c=cal_fld, s=size_fld,cmap=cmap, norm=norm, zorder=3, transform=ccrs.PlateCarree())\n",
    "        def color_bar(l,b,w,h):\n",
    "          rect = [l,b,w,h]\n",
    "          cbar_ax = fig.add_axes(rect)\n",
    "          return cbar_ax\n",
    "        [l2,b2,w2,h2] = [0.81,0.38,0.03,0.5]\n",
    "        cbar_ax2 = color_bar(l2,b2,w2,h2)\n",
    "        cb2 = plt.colorbar(con_plot2, cax=cbar_ax2,orientation=\"vertical\",shrink=0.5)\n",
    "        cb2.set_ticks([0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5])\n",
    "        cb2.set_ticklabels(['only Sim','only DA','only SAR','Sim+DA','Sim+SAR','DA+SAR','Sim+DA+SAR'],fontsize=8)\n",
    "    # 0.5: only sim, 1.5: only da, 2.5: only sat, 3.5: sim+da, 4.5: sim+sat, 5.5: da+sat, 6.5: sim+da+sat, nan: no flood\n",
    "    comp_fld(ax1,lon_ret1, lon_ret2,lat_ret1, lat_ret2, lon2d_sim1, lat2d_sim1, cal_fld1[:,:,0], size_fld1[:,:,0])\n",
    "    ax1.text(0.01, 0.95, 'Reg1',transform=ax1.transAxes,fontsize=10,verticalalignment='top', horizontalalignment='left',fontweight='bold')\n",
    "    comp_fld(ax2,lon_ret3, lon_ret4,lat_ret3, lat_ret4, lon2d_sim2, lat2d_sim2, cal_fld2[:,:,0], size_fld2[:,:,0])\n",
    "    ax2.text(0.01, 0.95, 'Reg2',transform=ax2.transAxes,fontsize=10,verticalalignment='top', horizontalalignment='left',fontweight='bold')\n",
    "    x = np.arange(1,1+np.shape(flood_da)[2],1)\n",
    "    ax3.plot(x,csi_da1,'r-',label='DA(Reg1)',lw=1.5)\n",
    "    ax3.plot(x,csi_sim1,'b-',label='Sim(Reg1)',lw=1.5)\n",
    "    ax3.plot(x,csi_da2,color='red',linestyle='--',label='DA(Reg2)',lw=1.5)\n",
    "    ax3.plot(x,csi_sim2,color='blue',linestyle='--',label='Sim(Reg2)',lw=1.5)\n",
    "    ax3.legend(loc='upper center', bbox_to_anchor=(0.55, 0.97), ncol=4, fontsize=7, frameon=False)\n",
    "    ax3.set_xlim(0.5,0.5+np.shape(flood_da)[2])\n",
    "    ax3.set_ylim(0,0.25)\n",
    "    ax3.set_ylabel('CSI',fontsize=10)\n",
    "    if np.shape(flood_da)[2]>20:\n",
    "        ax3.set_xticks(np.arange(1,1+np.shape(flood_da)[2],2))\n",
    "        ax3.set_xticklabels(np.arange(1,1+np.shape(flood_da)[2],2))\n",
    "    else:\n",
    "        ax3.set_xticks(np.arange(1,1+np.shape(flood_da)[2]))\n",
    "    if 'ils03' in pm_path:\n",
    "        ax3.set_xticklabels(['3','6','9','12','15','18','21'])\n",
    "    if 'ils06' in pm_path:\n",
    "        ax3.set_xticklabels(['3','9','15','21'])\n",
    "    if 'ils12' in pm_path:\n",
    "        ax3.set_xticklabels(['9','21'])\n",
    "    ax3.set_xlabel('DA Time Before SAR Observation (h)',fontsize=10)\n",
    "    plt.savefig(exp_plotdir+'6.24hour_flood_region.jpg', format='jpg',dpi=600)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "plot_flood_condition(csi_da1,csi_sim1,csi_da2,csi_sim2)\n",
    "plot_flood_condition(csi_da_grid1,csi_sim_grid1,csi_da_grid2,csi_sim_grid2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd62be8c-dfe1-4e0b-9368-b29901c9e39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% plot flood condition for 1-24 hours ahead\n",
    "def plot_flood_condition_all():\n",
    "    fig, axes = plt.subplots(8, 6, figsize=(36, 24), dpi=600,subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "    axes = axes.flatten()\n",
    "    def comp_fld(ax1, lon_ret1, lon_ret2,lat_ret1, lat_ret2, lon2d_sim, lat2d_sim, cal_fld, size_fld, plot_idx):\n",
    "        ax1.set_extent([lon_ret1, lon_ret2,lat_ret1, lat_ret2], ccrs.PlateCarree())\n",
    "        gl = ax1.gridlines(crs=ccrs.PlateCarree(), draw_labels=True,\n",
    "                        linewidth=1, color='gray', alpha=0, linestyle='-.')\n",
    "        ax1.coastlines(alpha=1.,linestyle='-',color = \"#3F3A3A\",lw=0.8,resolution='10m')\n",
    "        cmap_blue = ListedColormap(['blue']) \n",
    "        colors = plt.cm.rainbow(np.linspace(0, 1, 7)) \n",
    "        cmap = ListedColormap(colors) \n",
    "        bounds = np.arange(0, 8)\n",
    "        norm = BoundaryNorm(bounds, cmap.N) \n",
    "        gl.top_labels   = False\n",
    "        gl.right_labels = False\n",
    "        gl.bottom_labels= False\n",
    "        gl.left_labels = False\n",
    "        lon_map = np.linspace(lon_min,lon_max,int((lon_max-lon_min)*3600))\n",
    "        lat_map = np.linspace(lat_min,lat_max,int((lat_max-lat_min)*3600))\n",
    "        lon_tiff_ind = np.where((lon_map>=lon_ret1)&(lon_map<=lon_ret2))[0]\n",
    "        lat_tiff_ind = np.where((lat_map>=lat_ret1)&(lat_map<=lat_ret2))[0]\n",
    "        lat_tiff_reg = lat_map[np.nanmin(lat_tiff_ind):np.nanmax(lat_tiff_ind)+1]\n",
    "        lon_tiff_reg = lon_map[np.nanmin(lon_tiff_ind):np.nanmax(lon_tiff_ind)+1]\n",
    "        lon2d_tiff, lat2d_tiff = np.meshgrid(lon_tiff_reg, lat_tiff_reg)\n",
    "        uparea_reg = up_area[np.nanmin(lat_tiff_ind):np.nanmax(lat_tiff_ind)+1,np.nanmin(lon_tiff_ind):np.nanmax(lon_tiff_ind)+1]\n",
    "        uparea_reg = np.where(uparea_reg<10,np.nan,0.01)\n",
    "        ax1.scatter(lon2d_tiff, lat2d_tiff,uparea_reg, c='blue',zorder=3, transform=ccrs.PlateCarree())\n",
    "        ## ax1.pcolormesh(lon_tiff_reg,lat_tiff_reg,uparea_reg,zorder=2, cmap=cmap_blue, transform=ccrs.PlateCarree())  \n",
    "        con_plot2 = ax1.scatter(lon2d_sim, lat2d_sim, c=cal_fld, s=size_fld,cmap=cmap, norm=norm, zorder=3, transform=ccrs.PlateCarree())\n",
    "        if plot_idx == 0:\n",
    "            def color_bar(l,b,w,h):\n",
    "              rect = [l,b,w,h]\n",
    "              cbar_ax = fig.add_axes(rect)\n",
    "              return cbar_ax\n",
    "            [l2,b2,w2,h2] = [0.85,0.28,0.03,0.75]\n",
    "            cbar_ax2 = color_bar(l2,b2,w2,h2)\n",
    "            cb2 = plt.colorbar(con_plot2, cax=cbar_ax2,orientation=\"vertical\",shrink=0.5)\n",
    "            cb2.set_ticks([0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5])\n",
    "            cb2.set_ticklabels(['only Sim','only DA','only SAR','Sim+DA','Sim+SAR','DA+SAR','Sim+DA+SAR'],fontdict=font_label)\n",
    "    # 0.5: only sim, 1.5: only da, 2.5: only sat, 3.5: sim+da, 4.5: sim+sat, 5.5: da+sat, 6.5: sim+da+sat, nan: no flood\n",
    "    for plot_idx in range(0,23):\n",
    "        comp_fld(axes[plot_idx],lon_ret1, lon_ret2,lat_ret1, lat_ret2, lon2d_sim1, lat2d_sim1, cal_fld1[:,:,plot_idx], size_fld1[:,:,plot_idx], plot_idx)\n",
    "    axes[24-1].axis('off')\n",
    "    for plot_idx in range(0,23):\n",
    "        comp_fld(axes[plot_idx+24],lon_ret3, lon_ret4,lat_ret3, lat_ret4, lon2d_sim2, lat2d_sim2, cal_fld2[:,:,plot_idx], size_fld2[:,:,plot_idx], plot_idx)\n",
    "    axes[48-1].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    # plt.savefig('/work/a06/yingying/Code/plot/online/jupyter/exp_'+exp+dahour_str+'/'+'2_flood'+yyyymmddhh[0:8]+'.jpg', format='jpg',dpi=600)\n",
    "    plt.close()\n",
    "# plot_flood_condition_all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b49a12b-1408-43f2-bea5-2dcd8c917597",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867bfdb7-60c8-4463-a350-c5b12dbe8f33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4622a83f-b6d2-4eef-a1f6-78c5b369f013",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a2bb99-91a0-4c33-a7a4-7e85bcf94414",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data",
   "language": "python",
   "name": "data"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
